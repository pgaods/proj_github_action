{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we embark on the journey of studying FLAML, the auto-ML library developed by Microsoft."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ray==2.38.0 in c:\\users\\pgao\\.conda\\envs\\python310_uat\\lib\\site-packages (2.38.0)\n",
      "Requirement already satisfied: rgf-python==3.12.0 in c:\\users\\pgao\\.conda\\envs\\python310_uat\\lib\\site-packages (3.12.0)\n",
      "Requirement already satisfied: click>=7.0 in c:\\users\\pgao\\.conda\\envs\\python310_uat\\lib\\site-packages (from ray==2.38.0) (8.1.7)\n",
      "Requirement already satisfied: filelock in c:\\users\\pgao\\.conda\\envs\\python310_uat\\lib\\site-packages (from ray==2.38.0) (3.16.1)\n",
      "Requirement already satisfied: jsonschema in c:\\users\\pgao\\appdata\\roaming\\python\\python310\\site-packages (from ray==2.38.0) (4.18.3)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in c:\\users\\pgao\\.conda\\envs\\python310_uat\\lib\\site-packages (from ray==2.38.0) (1.1.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\pgao\\appdata\\roaming\\python\\python310\\site-packages (from ray==2.38.0) (23.1)\n",
      "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in c:\\users\\pgao\\.conda\\envs\\python310_uat\\lib\\site-packages (from ray==2.38.0) (4.24.4)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\pgao\\appdata\\roaming\\python\\python310\\site-packages (from ray==2.38.0) (6.0)\n",
      "Requirement already satisfied: aiosignal in c:\\users\\pgao\\.conda\\envs\\python310_uat\\lib\\site-packages (from ray==2.38.0) (1.3.1)\n",
      "Requirement already satisfied: frozenlist in c:\\users\\pgao\\.conda\\envs\\python310_uat\\lib\\site-packages (from ray==2.38.0) (1.5.0)\n",
      "Requirement already satisfied: requests in c:\\users\\pgao\\.conda\\envs\\python310_uat\\lib\\site-packages (from ray==2.38.0) (2.28.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\pgao\\appdata\\roaming\\python\\python310\\site-packages (from rgf-python==3.12.0) (1.3.2)\n",
      "Requirement already satisfied: scikit-learn>=0.18 in c:\\users\\pgao\\appdata\\roaming\\python\\python310\\site-packages (from rgf-python==3.12.0) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\pgao\\appdata\\roaming\\python\\python310\\site-packages (from click>=7.0->ray==2.38.0) (0.4.6)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\pgao\\appdata\\roaming\\python\\python310\\site-packages (from scikit-learn>=0.18->rgf-python==3.12.0) (1.25.2)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\pgao\\appdata\\roaming\\python\\python310\\site-packages (from scikit-learn>=0.18->rgf-python==3.12.0) (1.11.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\pgao\\appdata\\roaming\\python\\python310\\site-packages (from scikit-learn>=0.18->rgf-python==3.12.0) (3.2.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\pgao\\appdata\\roaming\\python\\python310\\site-packages (from jsonschema->ray==2.38.0) (23.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\pgao\\appdata\\roaming\\python\\python310\\site-packages (from jsonschema->ray==2.38.0) (2023.6.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\pgao\\appdata\\roaming\\python\\python310\\site-packages (from jsonschema->ray==2.38.0) (0.29.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\pgao\\appdata\\roaming\\python\\python310\\site-packages (from jsonschema->ray==2.38.0) (0.8.11)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\pgao\\.conda\\envs\\python310_uat\\lib\\site-packages (from requests->ray==2.38.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\pgao\\appdata\\roaming\\python\\python310\\site-packages (from requests->ray==2.38.0) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\pgao\\.conda\\envs\\python310_uat\\lib\\site-packages (from requests->ray==2.38.0) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pgao\\.conda\\envs\\python310_uat\\lib\\site-packages (from requests->ray==2.38.0) (2022.12.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install ray==2.38.0 rgf-python==3.12.0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: flaml==2.3.2 in c:\\users\\pgao\\.conda\\envs\\python310_uat\\lib\\site-packages (from flaml[automl]==2.3.2) (2.3.2)\n",
      "Requirement already satisfied: NumPy>=1.17 in c:\\users\\pgao\\appdata\\roaming\\python\\python310\\site-packages (from flaml==2.3.2->flaml[automl]==2.3.2) (1.25.2)\n",
      "Requirement already satisfied: lightgbm>=2.3.1 in c:\\users\\pgao\\.conda\\envs\\python310_uat\\lib\\site-packages (from flaml[automl]==2.3.2) (4.5.0)\n",
      "Requirement already satisfied: xgboost<3.0.0,>=0.90 in c:\\users\\pgao\\.conda\\envs\\python310_uat\\lib\\site-packages (from flaml[automl]==2.3.2) (2.1.2)\n",
      "Requirement already satisfied: scipy>=1.4.1 in c:\\users\\pgao\\appdata\\roaming\\python\\python310\\site-packages (from flaml[automl]==2.3.2) (1.11.2)\n",
      "Requirement already satisfied: pandas>=1.1.4 in c:\\users\\pgao\\appdata\\roaming\\python\\python310\\site-packages (from flaml[automl]==2.3.2) (2.1.0)\n",
      "Requirement already satisfied: scikit-learn>=1.0.0 in c:\\users\\pgao\\appdata\\roaming\\python\\python310\\site-packages (from flaml[automl]==2.3.2) (1.3.0)\n",
      "Requirement already satisfied: optuna<=3.6.1,>=2.8.0 in c:\\users\\pgao\\.conda\\envs\\python310_uat\\lib\\site-packages (from flaml[blendsearch]==2.3.2) (3.6.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\pgao\\appdata\\roaming\\python\\python310\\site-packages (from flaml[blendsearch]==2.3.2) (23.1)\n",
      "Requirement already satisfied: alembic>=1.5.0 in c:\\users\\pgao\\.conda\\envs\\python310_uat\\lib\\site-packages (from optuna<=3.6.1,>=2.8.0->flaml[blendsearch]==2.3.2) (1.14.0)\n",
      "Requirement already satisfied: colorlog in c:\\users\\pgao\\.conda\\envs\\python310_uat\\lib\\site-packages (from optuna<=3.6.1,>=2.8.0->flaml[blendsearch]==2.3.2) (6.9.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.3.0 in c:\\users\\pgao\\.conda\\envs\\python310_uat\\lib\\site-packages (from optuna<=3.6.1,>=2.8.0->flaml[blendsearch]==2.3.2) (2.0.36)\n",
      "Requirement already satisfied: tqdm in c:\\users\\pgao\\.conda\\envs\\python310_uat\\lib\\site-packages (from optuna<=3.6.1,>=2.8.0->flaml[blendsearch]==2.3.2) (4.64.1)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\pgao\\appdata\\roaming\\python\\python310\\site-packages (from optuna<=3.6.1,>=2.8.0->flaml[blendsearch]==2.3.2) (6.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\pgao\\appdata\\roaming\\python\\python310\\site-packages (from pandas>=1.1.4->flaml[automl]==2.3.2) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\pgao\\appdata\\roaming\\python\\python310\\site-packages (from pandas>=1.1.4->flaml[automl]==2.3.2) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\pgao\\appdata\\roaming\\python\\python310\\site-packages (from pandas>=1.1.4->flaml[automl]==2.3.2) (2023.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\pgao\\appdata\\roaming\\python\\python310\\site-packages (from scikit-learn>=1.0.0->flaml[automl]==2.3.2) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\pgao\\appdata\\roaming\\python\\python310\\site-packages (from scikit-learn>=1.0.0->flaml[automl]==2.3.2) (3.2.0)\n",
      "Requirement already satisfied: Mako in c:\\users\\pgao\\.conda\\envs\\python310_uat\\lib\\site-packages (from alembic>=1.5.0->optuna<=3.6.1,>=2.8.0->flaml[blendsearch]==2.3.2) (1.3.6)\n",
      "Requirement already satisfied: typing-extensions>=4 in c:\\users\\pgao\\appdata\\roaming\\python\\python310\\site-packages (from alembic>=1.5.0->optuna<=3.6.1,>=2.8.0->flaml[blendsearch]==2.3.2) (4.7.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\pgao\\appdata\\roaming\\python\\python310\\site-packages (from python-dateutil>=2.8.2->pandas>=1.1.4->flaml[automl]==2.3.2) (1.16.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\pgao\\.conda\\envs\\python310_uat\\lib\\site-packages (from sqlalchemy>=1.3.0->optuna<=3.6.1,>=2.8.0->flaml[blendsearch]==2.3.2) (3.1.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\pgao\\appdata\\roaming\\python\\python310\\site-packages (from colorlog->optuna<=3.6.1,>=2.8.0->flaml[blendsearch]==2.3.2) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in c:\\users\\pgao\\appdata\\roaming\\python\\python310\\site-packages (from Mako->alembic>=1.5.0->optuna<=3.6.1,>=2.8.0->flaml[blendsearch]==2.3.2) (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install flaml[automl]==2.3.2 flaml[blendsearch]==2.3.2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install flaml[ray]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PGAO\\.conda\\envs\\python310_uat\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-11-13 19:43:25,767\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2024-11-13 19:43:26,307\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "Tried to attach usage logger `pyspark.databricks.pandas.usage_logger`, but an exception was raised: JVM wasn't initialised. Did you call it on executor side?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.2\n",
      "1.3.0\n",
      "3.12.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os, sys, time, datetime, loguru\n",
    "import flaml, sklearn, pickle, rgf\n",
    "\n",
    "from flaml import AutoML, tune\n",
    "from flaml.automl.model import SKLearnEstimator # SKLearnEstimator is derived from BaseEstimator\n",
    "from flaml.automl.data import get_output_from_log\n",
    "# from ray import tune\n",
    "\n",
    "from IPython.display import Image\n",
    "from matplotlib.patches import Patch\n",
    "from rgf.sklearn import RGFClassifier, RGFRegressor \n",
    "from sklearn.datasets import load_iris, fetch_california_housing, fetch_openml\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, log_loss, f1_score, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(flaml.__version__) # 2.3.2\n",
    "print(sklearn.__version__) # 1.3.0\n",
    "print(rgf.__version__) # 3.12.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COMPUTE REQUIREMENTS\n",
    "\n",
    "Any UC-enabled cluster from Databricks will work. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. A Warm Start on FLAML\n",
    "\n",
    "The main class we will be using is `flaml.AutoML`, followed by a standard fit() method and a set of methods of prediction such as predict() and predict_proba() etc..\n",
    "\n",
    "Let's use the Iris data as an example to allow FLAML to automatically select the best model given a time constraint (denoted by the \"time_budget\" key-value pair in the `automl_settings` dictionary below):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "automl_settings = {\n",
    "    \"time_budget\": 30,  # in seconds\n",
    "    \"metric\": \"accuracy\",\n",
    "    \"task\": \"classification\",\n",
    "    \"log_file_name\": \"iris.log\",\n",
    "    \"verbose\": True,\n",
    "}\n",
    "\n",
    "X, y= load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "automl_model = AutoML()\n",
    "automl_model.fit(X_train, y_train, **automl_settings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy on Test Data:  0.9666666666666667\n",
      "Model selected by FLAML:  SGDClassifier(alpha=0.0004822000992733533, eta0=0.007328089893072002,\n",
      "              learning_rate='constant', loss='log_loss', n_jobs=-1,\n",
      "              penalty='l1', tol=0.0001)\n"
     ]
    }
   ],
   "source": [
    "y_hat = automl_model.predict(X_test)\n",
    "# print(automl_model.predict_proba(X_test))\n",
    "accuracy = sklearn.metrics.accuracy_score(y_test, y_hat)\n",
    "print(\"Model Accuracy on Test Data: \", accuracy)\n",
    "print(\"Model selected by FLAML: \", automl_model.model.estimator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. Task Oriented AutoML\n",
    "\n",
    "The class `flaml.AutoML` is a class for task-oriented AutoML. It can be used as a scikit-learn style estimator with the standard fit() and predict() functions. The minimal inputs from users are the training data and the task type.\n",
    "\n",
    "The training data can either by NumPy arrays or Pandas DataFrame objects (cannot be PySpark at this moment). \n",
    "\n",
    "The tasks include the following:\n",
    "\n",
    "   - 'classification': classification with tabular data;\n",
    "   - 'regression': regression with tabular data;\n",
    "   - 'ts_forecast': time series forecasting;\n",
    "   - 'ts_forecast_classification': time series forecasting for classification;\n",
    "   - 'ts_forecast_panel': time series forecasting for panel datasets (multiple time series);\n",
    "   - 'rank': learning to rank;\n",
    "   - 'seq-classification': sequence classification;\n",
    "   - 'seq-regression': sequence regression;\n",
    "   - 'summarization': text summarization;\n",
    "   - 'token-classification': token classification;\n",
    "   - 'multichoice-classification': multichoice classification.\n",
    "\n",
    "Two optional inputs are _time\\_budget_ and _max\\_iter_ for searching models and hyperparameters. When both are unspecified, only one model per estimator will be trained.\n",
    "\n",
    "We can pickle the model like below. And then later reload the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "automl_settings = {\n",
    "    \"time_budget\": 30,  # in seconds\n",
    "    \"metric\": \"mse\",\n",
    "    \"task\": \"regression\",\n",
    "    \"log_file_name\": \"C:\\\\Users\\\\PGAO\\\\GAO\\\\logs\\\\california_housing.log\",}\n",
    "artifact_location = \"C:\\\\Users\\\\PGAO\\\\GAO\\Python_notes\"\n",
    "model_name = \"automl.pkl\"\n",
    "model_path = os.path.join(artifact_location, model_name)\n",
    "\n",
    "X, y = fetch_california_housing(return_X_y=True, as_frame=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.automl.logger: 11-13 19:45:07] {1728} INFO - task = regression\n",
      "[flaml.automl.logger: 11-13 19:45:07] {1739} INFO - Evaluation method: holdout\n",
      "[flaml.automl.logger: 11-13 19:45:07] {1838} INFO - Minimizing error metric: mse\n",
      "[flaml.automl.logger: 11-13 19:45:07] {1955} INFO - List of ML learners in AutoML Run: ['lgbm', 'rf', 'xgboost', 'extra_tree', 'xgb_limitdepth', 'sgd', 'catboost']\n",
      "[flaml.automl.logger: 11-13 19:45:07] {2258} INFO - iteration 0, current learner lgbm\n",
      "[flaml.automl.logger: 11-13 19:45:07] {2393} INFO - Estimated sufficient time budget=445s. Estimated necessary time budget=4s.\n",
      "[flaml.automl.logger: 11-13 19:45:07] {2442} INFO -  at 0.1s,\testimator lgbm's best error=0.9861,\tbest estimator lgbm's best error=0.9861\n",
      "[flaml.automl.logger: 11-13 19:45:07] {2258} INFO - iteration 1, current learner lgbm\n",
      "[flaml.automl.logger: 11-13 19:45:07] {2442} INFO -  at 0.2s,\testimator lgbm's best error=0.9861,\tbest estimator lgbm's best error=0.9861\n",
      "[flaml.automl.logger: 11-13 19:45:07] {2258} INFO - iteration 2, current learner sgd\n",
      "[flaml.automl.logger: 11-13 19:45:07] {2442} INFO -  at 0.2s,\testimator sgd's best error=1.4087,\tbest estimator lgbm's best error=0.9861\n",
      "[flaml.automl.logger: 11-13 19:45:07] {2258} INFO - iteration 3, current learner sgd\n",
      "[flaml.automl.logger: 11-13 19:45:07] {2442} INFO -  at 0.2s,\testimator sgd's best error=1.4087,\tbest estimator lgbm's best error=0.9861\n",
      "[flaml.automl.logger: 11-13 19:45:07] {2258} INFO - iteration 4, current learner lgbm\n",
      "[flaml.automl.logger: 11-13 19:45:07] {2442} INFO -  at 0.3s,\testimator lgbm's best error=0.7108,\tbest estimator lgbm's best error=0.7108\n",
      "[flaml.automl.logger: 11-13 19:45:07] {2258} INFO - iteration 5, current learner xgboost\n",
      "[flaml.automl.logger: 11-13 19:45:07] {2442} INFO -  at 0.5s,\testimator xgboost's best error=0.9903,\tbest estimator lgbm's best error=0.7108\n",
      "[flaml.automl.logger: 11-13 19:45:07] {2258} INFO - iteration 6, current learner xgboost\n",
      "[flaml.automl.logger: 11-13 19:45:07] {2442} INFO -  at 0.6s,\testimator xgboost's best error=0.9903,\tbest estimator lgbm's best error=0.7108\n",
      "[flaml.automl.logger: 11-13 19:45:07] {2258} INFO - iteration 7, current learner lgbm\n",
      "[flaml.automl.logger: 11-13 19:45:07] {2442} INFO -  at 0.7s,\testimator lgbm's best error=0.3579,\tbest estimator lgbm's best error=0.3579\n",
      "[flaml.automl.logger: 11-13 19:45:07] {2258} INFO - iteration 8, current learner extra_tree\n",
      "[flaml.automl.logger: 11-13 19:45:08] {2442} INFO -  at 1.0s,\testimator extra_tree's best error=0.7662,\tbest estimator lgbm's best error=0.3579\n",
      "[flaml.automl.logger: 11-13 19:45:08] {2258} INFO - iteration 9, current learner rf\n",
      "[flaml.automl.logger: 11-13 19:45:08] {2442} INFO -  at 1.7s,\testimator rf's best error=0.6815,\tbest estimator lgbm's best error=0.3579\n",
      "[flaml.automl.logger: 11-13 19:45:08] {2258} INFO - iteration 10, current learner lgbm\n",
      "[flaml.automl.logger: 11-13 19:45:09] {2442} INFO -  at 2.0s,\testimator lgbm's best error=0.3579,\tbest estimator lgbm's best error=0.3579\n",
      "[flaml.automl.logger: 11-13 19:45:09] {2258} INFO - iteration 11, current learner lgbm\n",
      "[flaml.automl.logger: 11-13 19:45:09] {2442} INFO -  at 2.4s,\testimator lgbm's best error=0.3274,\tbest estimator lgbm's best error=0.3274\n",
      "[flaml.automl.logger: 11-13 19:45:09] {2258} INFO - iteration 12, current learner xgboost\n",
      "[flaml.automl.logger: 11-13 19:45:09] {2442} INFO -  at 2.5s,\testimator xgboost's best error=0.7198,\tbest estimator lgbm's best error=0.3274\n",
      "[flaml.automl.logger: 11-13 19:45:09] {2258} INFO - iteration 13, current learner extra_tree\n",
      "[flaml.automl.logger: 11-13 19:45:10] {2442} INFO -  at 2.8s,\testimator extra_tree's best error=0.5697,\tbest estimator lgbm's best error=0.3274\n",
      "[flaml.automl.logger: 11-13 19:45:10] {2258} INFO - iteration 14, current learner sgd\n",
      "[flaml.automl.logger: 11-13 19:45:10] {2442} INFO -  at 3.0s,\testimator sgd's best error=1.4087,\tbest estimator lgbm's best error=0.3274\n",
      "[flaml.automl.logger: 11-13 19:45:10] {2258} INFO - iteration 15, current learner xgboost\n",
      "[flaml.automl.logger: 11-13 19:45:10] {2442} INFO -  at 3.2s,\testimator xgboost's best error=0.4943,\tbest estimator lgbm's best error=0.3274\n",
      "[flaml.automl.logger: 11-13 19:45:10] {2258} INFO - iteration 16, current learner extra_tree\n",
      "[flaml.automl.logger: 11-13 19:45:10] {2442} INFO -  at 3.6s,\testimator extra_tree's best error=0.5697,\tbest estimator lgbm's best error=0.3274\n",
      "[flaml.automl.logger: 11-13 19:45:10] {2258} INFO - iteration 17, current learner rf\n",
      "[flaml.automl.logger: 11-13 19:45:11] {2442} INFO -  at 4.6s,\testimator rf's best error=0.4983,\tbest estimator lgbm's best error=0.3274\n",
      "[flaml.automl.logger: 11-13 19:45:11] {2258} INFO - iteration 18, current learner sgd\n",
      "[flaml.automl.logger: 11-13 19:45:11] {2442} INFO -  at 4.6s,\testimator sgd's best error=1.3422,\tbest estimator lgbm's best error=0.3274\n",
      "[flaml.automl.logger: 11-13 19:45:11] {2258} INFO - iteration 19, current learner lgbm\n",
      "[flaml.automl.logger: 11-13 19:45:11] {2442} INFO -  at 4.8s,\testimator lgbm's best error=0.3274,\tbest estimator lgbm's best error=0.3274\n",
      "[flaml.automl.logger: 11-13 19:45:11] {2258} INFO - iteration 20, current learner xgboost\n",
      "[flaml.automl.logger: 11-13 19:45:12] {2442} INFO -  at 4.8s,\testimator xgboost's best error=0.4943,\tbest estimator lgbm's best error=0.3274\n",
      "[flaml.automl.logger: 11-13 19:45:12] {2258} INFO - iteration 21, current learner rf\n",
      "[flaml.automl.logger: 11-13 19:45:13] {2442} INFO -  at 6.7s,\testimator rf's best error=0.4983,\tbest estimator lgbm's best error=0.3274\n",
      "[flaml.automl.logger: 11-13 19:45:13] {2258} INFO - iteration 22, current learner xgboost\n",
      "[flaml.automl.logger: 11-13 19:45:13] {2442} INFO -  at 6.7s,\testimator xgboost's best error=0.4943,\tbest estimator lgbm's best error=0.3274\n",
      "[flaml.automl.logger: 11-13 19:45:13] {2258} INFO - iteration 23, current learner lgbm\n",
      "[flaml.automl.logger: 11-13 19:45:13] {2442} INFO -  at 6.8s,\testimator lgbm's best error=0.3274,\tbest estimator lgbm's best error=0.3274\n",
      "[flaml.automl.logger: 11-13 19:45:13] {2258} INFO - iteration 24, current learner lgbm\n",
      "[flaml.automl.logger: 11-13 19:45:14] {2442} INFO -  at 7.4s,\testimator lgbm's best error=0.2858,\tbest estimator lgbm's best error=0.2858\n",
      "[flaml.automl.logger: 11-13 19:45:14] {2258} INFO - iteration 25, current learner catboost\n",
      "[flaml.automl.logger: 11-13 19:45:21] {2442} INFO -  at 14.2s,\testimator catboost's best error=0.2036,\tbest estimator catboost's best error=0.2036\n",
      "[flaml.automl.logger: 11-13 19:45:21] {2258} INFO - iteration 26, current learner extra_tree\n",
      "[flaml.automl.logger: 11-13 19:45:21] {2442} INFO -  at 14.5s,\testimator extra_tree's best error=0.4929,\tbest estimator catboost's best error=0.2036\n",
      "[flaml.automl.logger: 11-13 19:45:21] {2258} INFO - iteration 27, current learner catboost\n",
      "[flaml.automl.logger: 11-13 19:45:35] {2442} INFO -  at 28.7s,\testimator catboost's best error=0.2033,\tbest estimator catboost's best error=0.2033\n",
      "[flaml.automl.logger: 11-13 19:45:35] {2258} INFO - iteration 28, current learner xgboost\n",
      "[flaml.automl.logger: 11-13 19:45:36] {2442} INFO -  at 28.9s,\testimator xgboost's best error=0.3707,\tbest estimator catboost's best error=0.2033\n",
      "[flaml.automl.logger: 11-13 19:45:36] {2258} INFO - iteration 29, current learner xgboost\n",
      "[flaml.automl.logger: 11-13 19:45:36] {2442} INFO -  at 29.0s,\testimator xgboost's best error=0.3224,\tbest estimator catboost's best error=0.2033\n",
      "[flaml.automl.logger: 11-13 19:45:36] {2258} INFO - iteration 30, current learner extra_tree\n",
      "[flaml.automl.logger: 11-13 19:45:36] {2442} INFO -  at 29.3s,\testimator extra_tree's best error=0.4040,\tbest estimator catboost's best error=0.2033\n",
      "[flaml.automl.logger: 11-13 19:45:36] {2258} INFO - iteration 31, current learner extra_tree\n",
      "[flaml.automl.logger: 11-13 19:45:36] {2442} INFO -  at 29.5s,\testimator extra_tree's best error=0.4040,\tbest estimator catboost's best error=0.2033\n",
      "[flaml.automl.logger: 11-13 19:45:36] {2258} INFO - iteration 32, current learner xgboost\n",
      "[flaml.automl.logger: 11-13 19:45:36] {2442} INFO -  at 29.6s,\testimator xgboost's best error=0.3224,\tbest estimator catboost's best error=0.2033\n",
      "[flaml.automl.logger: 11-13 19:45:36] {2258} INFO - iteration 33, current learner xgboost\n",
      "[flaml.automl.logger: 11-13 19:45:36] {2442} INFO -  at 29.8s,\testimator xgboost's best error=0.2633,\tbest estimator catboost's best error=0.2033\n",
      "[flaml.automl.logger: 11-13 19:45:36] {2258} INFO - iteration 34, current learner xgboost\n",
      "[flaml.automl.logger: 11-13 19:45:37] {2442} INFO -  at 30.0s,\testimator xgboost's best error=0.2633,\tbest estimator catboost's best error=0.2033\n",
      "[flaml.automl.logger: 11-13 19:45:37] {2258} INFO - iteration 35, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-13 19:45:37] {2442} INFO -  at 30.0s,\testimator xgb_limitdepth's best error=0.8879,\tbest estimator catboost's best error=0.2033\n",
      "[flaml.automl.logger: 11-13 19:45:45] {2685} INFO - retrain catboost for 8.7s\n",
      "[flaml.automl.logger: 11-13 19:45:45] {2688} INFO - retrained model: <catboost.core.CatBoostRegressor object at 0x000001E769269E70>\n",
      "[flaml.automl.logger: 11-13 19:45:45] {1985} INFO - fit succeeded\n",
      "[flaml.automl.logger: 11-13 19:45:45] {1986} INFO - Time taken to find the best model: 28.742396593093872\n"
     ]
    }
   ],
   "source": [
    "automl = AutoML()\n",
    "automl.fit(X_train, y_train, **automl_settings)\n",
    "with open(model_path, \"wb\") as saved_model: # saving the model as a pickle file\n",
    "    pickle.dump(automl, saved_model, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metric on Test Data:  0.4510529358672329\n",
      "Model selected by FLAML:  SGDClassifier(alpha=0.0004822000992733533, eta0=0.007328089893072002,\n",
      "              learning_rate='constant', loss='log_loss', n_jobs=-1,\n",
      "              penalty='l1', tol=0.0001)\n"
     ]
    }
   ],
   "source": [
    "with open(model_path, \"rb\") as f: # loading the model from the pickle file\n",
    "    automl = pickle.load(f)\n",
    "y_hat = automl.predict(X_test)\n",
    "\n",
    "eval_metric = mean_squared_error(y_test, y_hat, squared=False)\n",
    "print(\"Evaluation Metric on Test Data: \", eval_metric)\n",
    "print(\"Model selected by FLAML: \", automl_model.model.estimator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimization Metrics\n",
    "\n",
    "For the fit() method, the optimization metric is specified via the _metric_ argument. It can be either a string which refers to a built-in metric, or a user-defined function. Common metrics include accuracy, log_loss, r2, rmse, mse, mae, mape, roc_auc, f1, micro_f1, macro_f1, ap (average precision) and ndcg score etc.. The ndcg metric stands for normalized discounted cumulative gain which measures the effectiveness of a ranking system by evaluating how well it ranks relevant items. A complete list of the metrics can be found here:\n",
    "\n",
    "   - https://microsoft.github.io/FLAML/docs/Use-Cases/Task-Oriented-AutoML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimators/Models\n",
    "\n",
    "The estimator list can contain one or more estimator names, each corresponding to a built-in estimator or a custom estimator. Each estimator has a search space for hyperparameter configurations. FLAML supports both classical machine learning models such as logit, various boosting models, tree-based models, arima, sarimax, holt-winters(triple exponential smoothing), prophet, and deep neural networks. Again, the complete list can be found here: \n",
    "\n",
    "   - https://microsoft.github.io/FLAML/docs/Use-Cases/Task-Oriented-AutoML/\n",
    "\n",
    "If a model is not included in the built-in estimators or models of FLAML, we can build one. For example, we know that the regularized greedy forest (RGF) is an estimator not included in RGF. Think of it like a team of decision trees working together to make better decisions. RGF is suitable for both classification and regression, and we don't need to create dummy variables ourselves. Normalizing data is not necessary but may be helpful for performance. Below in the next paragraph, we give a simple breakdown of how RGF works in layman's term. The original RGF paper can be retrieved here: \n",
    "\n",
    "   - https://arxiv.org/abs/1109.0887.\n",
    "\n",
    "   1. Decision Trees: imagine a decision tree as a flowchart that helps you make decisions by asking a series of questions. Each question splits the data into smaller groups based on the answers.\n",
    "   2. Ensemble Learning: RGF combines multiple decision trees to improve accuracy. Instead of relying on a single tree, it uses a “forest” of trees, which helps to make more reliable predictions.\n",
    "   3. Regularization: this is a technique to prevent the model from becoming too complex and overfitting the data (i.e., performing well on training data but poorly on new data). RGF uses regularization to keep the model simple and generalize better to new data.\n",
    "   4. Greedy Approach: the algorithm builds the forest step-by-step, making the best possible decision at each step to minimize errors. It adjusts the structure of the forest and the weights of the trees to improve performance.\n",
    "\n",
    "\n",
    "Let's now tune a custom estimator using RGF. Before doing that, let's first look at the original code of RGF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PGAO\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\datasets\\_openml.py:1002: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "titanic = fetch_openml('titanic', version=1, as_frame=True)\n",
    "data = titanic.frame\n",
    "features = [\"pclass\", \"sex\", \"sibsp\", \"parch\", \"embarked\"]\n",
    "target = \"survived\"\n",
    "X = data[features]\n",
    "y = data[target]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "X_train = pd.get_dummies(X_train[features])\n",
    "X_test  = pd.get_dummies(X_test[features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PGAO\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\datasets\\_openml.py:1002: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The evaluation score is 0.78244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PGAO\\.conda\\envs\\python310_uat\\lib\\site-packages\\rgf\\utils.py:224: UserWarning: Cannot find FastRGF executable files. FastRGF estimators will be unavailable for usage.\n",
      "  warnings.warn(\"Cannot find FastRGF executable files. \"\n"
     ]
    }
   ],
   "source": [
    "model = RGFClassifier(max_leaf=300, algorithm=\"RGF_Sib\", test_interval=100)\n",
    "model.fit(X_train, y_train)\n",
    "y_hat = model.predict(X_test)\n",
    "eval_metric = accuracy_score(y_test , y_hat)\n",
    "print(\"The evaluation score is %.5f\" % eval_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now use FLAML to do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from flaml.automl.model import SKLearnEstimator # SKLearnEstimator is derived from BaseEstimator\n",
    "class MyRegularizedGreedyForest(SKLearnEstimator):\n",
    "    \"\"\"\n",
    "    MyRegularizedGreedyForest is a custom estimator that integrates the Regularized Greedy Forest (RGF) algorithm\n",
    "    with FLAML's AutoML framework. It supports both classification and regression tasks.\n",
    "\n",
    "    Attributes:\n",
    "        estimator_class: The class of the estimator (RGFClassifier for classification tasks, RGFRegressor for regression tasks).\n",
    "\n",
    "    Methods:\n",
    "        __init__(task=\"binary\", **automl_config):\n",
    "            Initializes the estimator with the specified task and configuration.\n",
    "        \n",
    "        search_space(data_size, task):\n",
    "            Defines the hyperparameter search space for the estimator.\n",
    "    \"\"\"\n",
    "    # from rgf.sklearn import RGFClassifier, RGFRegressor\n",
    "    # import rgf\n",
    "\n",
    "    def __init__(self, task=\"binary\", **automl_config):\n",
    "        \"\"\"\n",
    "        Initializes the MyRegularizedGreedyForest instance.\n",
    "\n",
    "        Args:\n",
    "            task (str): The type of task, either 'CLASSIFICATION' or 'REGRESSION'.\n",
    "            **automl_config: Additional configuration parameters for the estimator.\n",
    "        \"\"\"\n",
    "        super().__init__(task, **automl_config)\n",
    "\n",
    "        if task == 'CLASSIFICATION':\n",
    "            self.estimator_class = RGFClassifier\n",
    "        else:\n",
    "            self.estimator_class = RGFRegressor\n",
    "\n",
    "    @classmethod\n",
    "    def search_space(cls, data_size, task):\n",
    "        \"\"\"\n",
    "        Defines the hyperparameter search space for the estimator.\n",
    "\n",
    "        Args:\n",
    "            data_size (int): The size of the dataset.\n",
    "            task (str): The type of task, either 'CLASSIFICATION' or 'REGRESSION'.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary representing the hyperparameter search space.\n",
    "        \"\"\"\n",
    "        space = {\n",
    "        \"max_leaf\": {\n",
    "            \"domain\": tune.lograndint(lower=4, upper=data_size),\n",
    "            \"low_cost_init_value\": 4,\n",
    "        },\n",
    "        \"n_iter\": {\n",
    "            \"domain\": tune.lograndint(lower=1, upper=data_size),\n",
    "            \"low_cost_init_value\": 1,\n",
    "        },\n",
    "        # \"learning_rate\": {\"domain\": tune.loguniform(lower=0.1, upper=20.0)},\n",
    "        \"min_samples_leaf\": {\n",
    "            \"domain\": tune.lograndint(lower=1, upper=20),\n",
    "            \"init_value\": 20,\n",
    "        },\n",
    "        }\n",
    "        return space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importantly, we also add the _task_=\"binary\" parameter in the signature of \\_\\_init\\_\\_ so that it doesn't get grouped together with the **config kwargs that determines the parameters with which the underlying estimator (`self.estimator_class`) is constructed.\n",
    "\n",
    "Now this registers the `MyRegularizedGreedyForest` class in AutoML, with the name \"rgf\" and then train/fit the model with automl.fit(X_train, y_train, estimator_list=[\"rgf\"]). For most of the times we don't need to make this problem too complicated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "automl = AutoML()\n",
    "automl.add_learner(\"rgf\", MyRegularizedGreedyForest) # adding a customized learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Search Space and Evaluation \n",
    "\n",
    "Each estimator class, built-in or not, must have a search_space() function. In the search_space() function, we return a dictionary about the hyperparameters, the keys of which are the names of the hyperparameters to tune, and each value is a set of detailed search configurations about the corresponding hyperparameters represented in a dictionary. A search configuration dictionary includes the following fields:\n",
    "\n",
    "  - \"domain\", which specifies the possible values of the hyperparameter and their distribution;\n",
    "  - \"init_value\" (optional), which specifies the initial value of the hyperparameter;\n",
    "  - \"low_cost_init_value\" (optional), which specifies the value of the hyperparameter that is associated with low computation cost. See cost related hyperparameters or FAQ for more details.\n",
    "\n",
    "The second step is to specify a search space of the hyperparameters through the argument _config_. In the search space, you need to specify valid values for your hyperparameters and can specify how these values are sampled (e.g., from a uniform distribution or a log-uniform distribution).\n",
    "\n",
    "But all of the above about search space is too complicated. There is a shortcut (recommended way) to change default hyperparameters for FLAML during fitting. One can use the _custom\\_hp_ argument in AutoML.fit() to override the search space for an existing estimator quickly. For example, if you would like to temporarily change the search range of \"n_estimators\" of xgboost, disable searching \"max_leaves\" in random forest, and add \"subsample\" in the search space of lightgbm, you can set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.automl.logger: 11-13 19:45:46] {1728} INFO - task = classification\n",
      "[flaml.automl.logger: 11-13 19:45:46] {1739} INFO - Evaluation method: cv\n",
      "[flaml.automl.logger: 11-13 19:45:46] {1838} INFO - Minimizing error metric: 1-roc_auc\n",
      "[flaml.automl.logger: 11-13 19:45:46] {1861} WARNING - No search budget is provided via time_budget or max_iter. Training only one model per estimator. Zero-shot AutoML is used for certain tasks and estimators. To tune hyperparameters for each estimator, please provide budget either via time_budget or max_iter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.automl.logger: 11-13 19:45:46] {1955} INFO - List of ML learners in AutoML Run: ['rf', 'lgbm', 'xgboost', 'extra_tree', 'xgb_limitdepth', 'sgd', 'catboost', 'lrl1']\n",
      "[flaml.automl.logger: 11-13 19:45:46] {2258} INFO - iteration 0, current learner rf\n",
      "[flaml.automl.logger: 11-13 19:45:50] {2393} INFO - Estimated sufficient time budget=10000s. Estimated necessary time budget=10s.\n",
      "[flaml.automl.logger: 11-13 19:45:50] {2442} INFO -  at 4.3s,\testimator rf's best error=0.1779,\tbest estimator rf's best error=0.1779\n",
      "[flaml.automl.logger: 11-13 19:45:50] {2258} INFO - iteration 1, current learner lgbm\n",
      "[flaml.automl.logger: 11-13 19:46:01] {2442} INFO -  at 14.9s,\testimator lgbm's best error=0.1670,\tbest estimator lgbm's best error=0.1670\n",
      "[flaml.automl.logger: 11-13 19:46:01] {2258} INFO - iteration 2, current learner xgboost\n",
      "[flaml.automl.logger: 11-13 19:54:22] {2442} INFO -  at 515.9s,\testimator xgboost's best error=0.1755,\tbest estimator lgbm's best error=0.1670\n",
      "[flaml.automl.logger: 11-13 19:54:22] {2258} INFO - iteration 3, current learner extra_tree\n",
      "[flaml.automl.logger: 11-13 19:54:27] {2442} INFO -  at 520.9s,\testimator extra_tree's best error=0.1801,\tbest estimator lgbm's best error=0.1670\n",
      "[flaml.automl.logger: 11-13 19:54:27] {2258} INFO - iteration 4, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-13 19:55:03] {2442} INFO -  at 557.5s,\testimator xgb_limitdepth's best error=0.1717,\tbest estimator lgbm's best error=0.1670\n",
      "[flaml.automl.logger: 11-13 19:55:03] {2258} INFO - iteration 5, current learner sgd\n",
      "[flaml.automl.logger: 11-13 19:55:04] {2442} INFO -  at 557.8s,\testimator sgd's best error=0.1942,\tbest estimator lgbm's best error=0.1670\n",
      "[flaml.automl.logger: 11-13 19:55:04] {2258} INFO - iteration 6, current learner catboost\n",
      "[flaml.automl.logger: 11-13 19:55:07] {2442} INFO -  at 561.3s,\testimator catboost's best error=0.1665,\tbest estimator catboost's best error=0.1665\n",
      "[flaml.automl.logger: 11-13 19:55:07] {2258} INFO - iteration 7, current learner lrl1\n",
      "[flaml.automl.logger: 11-13 19:55:08] {2442} INFO -  at 561.7s,\testimator lrl1's best error=0.1803,\tbest estimator catboost's best error=0.1665\n",
      "[flaml.automl.logger: 11-13 19:55:08] {2685} INFO - retrain catboost for 0.5s\n",
      "[flaml.automl.logger: 11-13 19:55:08] {2688} INFO - retrained model: <catboost.core.CatBoostClassifier object at 0x000001E76926B0D0>\n",
      "[flaml.automl.logger: 11-13 19:55:08] {1985} INFO - fit succeeded\n",
      "[flaml.automl.logger: 11-13 19:55:08] {1986} INFO - Time taken to find the best model: 561.326119184494\n",
      "0.7862595419847328\n"
     ]
    }
   ],
   "source": [
    "custom_hp = {\n",
    "    \"xgboost\": {\n",
    "        \"n_estimators\": {\n",
    "            \"domain\": tune.lograndint(lower=1, upper=12),\n",
    "            \"low_cost_init_value\": 5,\n",
    "        },\n",
    "    },\n",
    "    \"rf\": {\n",
    "        \"max_leaves\": {\n",
    "            \"domain\": None,  # disable search\n",
    "        },\n",
    "    },\n",
    "    \"lgbm\": {\n",
    "        \"subsample\": {\n",
    "            \"domain\": tune.uniform(lower=0.1, upper=2.0),\n",
    "            \"init_value\": 1.0,\n",
    "        },\n",
    "        \"subsample_freq\": {\n",
    "            \"domain\": 1,  # subsample_freq must > 0 to enable subsample\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "automl = AutoML()\n",
    "automl.fit(X_train, y_train, custom_hp=custom_hp)\n",
    "y_hat = automl.predict(X_test)\n",
    "eval_metric = sklearn.metrics.accuracy_score(y_test, y_hat)\n",
    "print(eval_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constraints on Model Training\n",
    "\n",
    "There are several times of constraints we can place on when we train the model. The most important 2 constraints are _time\\_budget_ and _max\\_iter_. \n",
    "\n",
    "Another way is to put constraints on the constructor arguments of the estimators through using the _custom\\_hp_ argument in the fit() method above. But we do not recommend doing this since sometimes the invalid parameters will take some time to error out in code.\n",
    "\n",
    "For example, below we put a constraint on the xgboost model so that we add monotonicity constraints. Please note that sometimes adding constraints may inherently make the model training take longer if we don't restrict time budgets:\n",
    "\n",
    "     > custom_hp = {\"xgboost\": {\"monotone_constraints\": {\"domain\": \"(your_value1, your_value2)\"}}}\n",
    "     > automl = AutoML()\n",
    "     > automl.fit(X_train, y_train, custom_hp=custom_hp, time_budget=30)\n",
    "\n",
    "Another way to put constraints is to use the other two important arguments in fit():\n",
    "\n",
    "   - _train\\_time\\_limit_: training time in seconds;\n",
    "   - _pred\\_time\\_limit_: prediction time per instance in seconds.\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.automl.logger: 11-13 19:55:08] {1728} INFO - task = classification\n",
      "[flaml.automl.logger: 11-13 19:55:08] {1739} INFO - Evaluation method: cv\n",
      "[flaml.automl.logger: 11-13 19:55:08] {1838} INFO - Minimizing error metric: 1-roc_auc\n",
      "[flaml.automl.logger: 11-13 19:55:08] {1955} INFO - List of ML learners in AutoML Run: ['lgbm', 'rf', 'xgboost', 'extra_tree', 'xgb_limitdepth', 'sgd', 'catboost', 'lrl1']\n",
      "[flaml.automl.logger: 11-13 19:55:08] {2258} INFO - iteration 0, current learner lgbm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.automl.logger: 11-13 19:55:09] {2393} INFO - Estimated sufficient time budget=10000s. Estimated necessary time budget=10s.\n",
      "[flaml.automl.logger: 11-13 19:55:09] {2442} INFO -  at 0.3s,\testimator lgbm's best error=0.1834,\tbest estimator lgbm's best error=0.1834\n",
      "[flaml.automl.logger: 11-13 19:55:09] {2258} INFO - iteration 1, current learner lgbm\n",
      "[flaml.automl.logger: 11-13 19:55:09] {2442} INFO -  at 0.5s,\testimator lgbm's best error=0.1834,\tbest estimator lgbm's best error=0.1834\n",
      "[flaml.automl.logger: 11-13 19:55:09] {2258} INFO - iteration 2, current learner lgbm\n",
      "[flaml.automl.logger: 11-13 19:55:09] {2442} INFO -  at 0.7s,\testimator lgbm's best error=0.1812,\tbest estimator lgbm's best error=0.1812\n",
      "[flaml.automl.logger: 11-13 19:55:09] {2258} INFO - iteration 3, current learner rf\n",
      "[flaml.automl.logger: 11-13 19:55:09] {2442} INFO -  at 1.1s,\testimator rf's best error=0.1910,\tbest estimator lgbm's best error=0.1812\n",
      "[flaml.automl.logger: 11-13 19:55:09] {2258} INFO - iteration 4, current learner lgbm\n",
      "[flaml.automl.logger: 11-13 19:55:10] {2442} INFO -  at 1.4s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
      "[flaml.automl.logger: 11-13 19:55:10] {2258} INFO - iteration 5, current learner lgbm\n",
      "[flaml.automl.logger: 11-13 19:55:10] {2442} INFO -  at 1.7s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
      "[flaml.automl.logger: 11-13 19:55:10] {2258} INFO - iteration 6, current learner lgbm\n",
      "[flaml.automl.logger: 11-13 19:55:10] {2442} INFO -  at 2.0s,\testimator lgbm's best error=0.1622,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 19:55:10] {2258} INFO - iteration 7, current learner lgbm\n",
      "[flaml.automl.logger: 11-13 19:55:11] {2442} INFO -  at 2.3s,\testimator lgbm's best error=0.1622,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 19:55:11] {2258} INFO - iteration 8, current learner lgbm\n",
      "[flaml.automl.logger: 11-13 19:55:11] {2442} INFO -  at 2.5s,\testimator lgbm's best error=0.1622,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 19:55:11] {2258} INFO - iteration 9, current learner rf\n",
      "[flaml.automl.logger: 11-13 19:55:11] {2442} INFO -  at 3.1s,\testimator rf's best error=0.1910,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 19:55:11] {2258} INFO - iteration 10, current learner rf\n",
      "[flaml.automl.logger: 11-13 19:55:12] {2442} INFO -  at 3.8s,\testimator rf's best error=0.1910,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 19:55:12] {2258} INFO - iteration 11, current learner xgboost\n",
      "[flaml.automl.logger: 11-13 19:55:12] {2442} INFO -  at 4.0s,\testimator xgboost's best error=0.1834,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 19:55:12] {2258} INFO - iteration 12, current learner xgboost\n",
      "[flaml.automl.logger: 11-13 19:55:13] {2442} INFO -  at 4.3s,\testimator xgboost's best error=0.1828,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 19:55:13] {2258} INFO - iteration 13, current learner extra_tree\n",
      "[flaml.automl.logger: 11-13 19:55:13] {2442} INFO -  at 4.7s,\testimator extra_tree's best error=0.1874,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 19:55:13] {2258} INFO - iteration 14, current learner rf\n",
      "[flaml.automl.logger: 11-13 19:55:13] {2442} INFO -  at 5.1s,\testimator rf's best error=0.1910,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 19:55:13] {2258} INFO - iteration 15, current learner extra_tree\n",
      "[flaml.automl.logger: 11-13 19:55:14] {2442} INFO -  at 5.5s,\testimator extra_tree's best error=0.1824,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 19:55:14] {2258} INFO - iteration 16, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-13 19:55:14] {2442} INFO -  at 6.0s,\testimator xgb_limitdepth's best error=0.1707,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 19:55:14] {2258} INFO - iteration 17, current learner lgbm\n",
      "[flaml.automl.logger: 11-13 19:55:15] {2442} INFO -  at 6.7s,\testimator lgbm's best error=0.1622,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 19:55:15] {2258} INFO - iteration 18, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-13 19:55:15] {2442} INFO -  at 7.1s,\testimator xgb_limitdepth's best error=0.1661,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 19:55:15] {2258} INFO - iteration 19, current learner sgd\n",
      "[flaml.automl.logger: 11-13 19:55:16] {2442} INFO -  at 7.4s,\testimator sgd's best error=0.1941,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 19:55:16] {2258} INFO - iteration 20, current learner lgbm\n",
      "[flaml.automl.logger: 11-13 19:55:16] {2442} INFO -  at 7.6s,\testimator lgbm's best error=0.1622,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 19:55:16] {2258} INFO - iteration 21, current learner sgd\n",
      "[flaml.automl.logger: 11-13 19:55:16] {2442} INFO -  at 7.7s,\testimator sgd's best error=0.1766,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 19:55:16] {2258} INFO - iteration 22, current learner catboost\n",
      "[flaml.automl.logger: 11-13 19:55:20] {2442} INFO -  at 11.4s,\testimator catboost's best error=0.1665,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 19:55:20] {2258} INFO - iteration 23, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-13 19:55:20] {2442} INFO -  at 12.1s,\testimator xgb_limitdepth's best error=0.1661,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 19:55:20] {2258} INFO - iteration 24, current learner sgd\n",
      "[flaml.automl.logger: 11-13 19:55:21] {2442} INFO -  at 12.3s,\testimator sgd's best error=0.1766,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 19:55:21] {2258} INFO - iteration 25, current learner rf\n",
      "[flaml.automl.logger: 11-13 19:55:21] {2442} INFO -  at 13.0s,\testimator rf's best error=0.1857,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 19:55:21] {2258} INFO - iteration 26, current learner extra_tree\n",
      "[flaml.automl.logger: 11-13 19:55:22] {2442} INFO -  at 13.7s,\testimator extra_tree's best error=0.1824,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 19:55:22] {2258} INFO - iteration 27, current learner catboost\n",
      "[flaml.automl.logger: 11-13 19:55:27] {2442} INFO -  at 19.0s,\testimator catboost's best error=0.1631,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 19:55:27] {2258} INFO - iteration 28, current learner catboost\n",
      "[flaml.automl.logger: 11-13 19:55:31] {2442} INFO -  at 23.0s,\testimator catboost's best error=0.1631,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 19:55:31] {2258} INFO - iteration 29, current learner lrl1\n",
      "[flaml.automl.logger: 11-13 19:55:32] {2442} INFO -  at 23.3s,\testimator lrl1's best error=0.1803,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 19:55:32] {2258} INFO - iteration 30, current learner sgd\n",
      "[flaml.automl.logger: 11-13 19:55:32] {2442} INFO -  at 23.4s,\testimator sgd's best error=0.1751,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 19:55:32] {2258} INFO - iteration 31, current learner lrl1\n",
      "[flaml.automl.logger: 11-13 19:55:32] {2442} INFO -  at 24.2s,\testimator lrl1's best error=0.1800,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 19:55:32] {2258} INFO - iteration 32, current learner catboost\n",
      "[flaml.automl.logger: 11-13 19:55:48] {2442} INFO -  at 39.6s,\testimator catboost's best error=0.1624,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 19:55:48] {2258} INFO - iteration 33, current learner catboost\n",
      "[flaml.automl.logger: 11-13 19:55:52] {2442} INFO -  at 44.2s,\testimator catboost's best error=0.1624,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 19:55:52] {2258} INFO - iteration 34, current learner lgbm\n",
      "[flaml.automl.logger: 11-13 19:55:53] {2442} INFO -  at 45.1s,\testimator lgbm's best error=0.1622,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 19:55:53] {2258} INFO - iteration 35, current learner lgbm\n",
      "[flaml.automl.logger: 11-13 19:55:54] {2442} INFO -  at 45.5s,\testimator lgbm's best error=0.1622,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 19:55:54] {2258} INFO - iteration 36, current learner rf\n",
      "[flaml.automl.logger: 11-13 19:55:54] {2442} INFO -  at 45.9s,\testimator rf's best error=0.1800,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 19:55:54] {2258} INFO - iteration 37, current learner catboost\n",
      "[flaml.automl.logger: 11-13 19:56:06] {2442} INFO -  at 57.6s,\testimator catboost's best error=0.1624,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 19:56:06] {2258} INFO - iteration 38, current learner lgbm\n",
      "[flaml.automl.logger: 11-13 19:56:07] {2442} INFO -  at 58.6s,\testimator lgbm's best error=0.1622,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 19:56:07] {2258} INFO - iteration 39, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-13 19:56:09] {2442} INFO -  at 60.4s,\testimator xgb_limitdepth's best error=0.1661,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 19:56:09] {2258} INFO - iteration 40, current learner catboost\n",
      "[flaml.automl.logger: 11-13 19:56:12] {2442} INFO -  at 64.0s,\testimator catboost's best error=0.1624,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 19:56:12] {2258} INFO - iteration 41, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-13 19:56:13] {2442} INFO -  at 64.5s,\testimator xgb_limitdepth's best error=0.1661,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 19:56:13] {2258} INFO - iteration 42, current learner catboost\n",
      "[flaml.automl.logger: 11-13 19:56:23] {2442} INFO -  at 75.1s,\testimator catboost's best error=0.1624,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 19:56:23] {2258} INFO - iteration 43, current learner rf\n",
      "[flaml.automl.logger: 11-13 19:56:24] {2442} INFO -  at 75.7s,\testimator rf's best error=0.1800,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 19:56:24] {2258} INFO - iteration 44, current learner catboost\n",
      "[flaml.automl.logger: 11-13 19:56:29] {2442} INFO -  at 81.1s,\testimator catboost's best error=0.1624,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 19:56:29] {2258} INFO - iteration 45, current learner catboost\n",
      "[flaml.automl.logger: 11-13 19:56:52] {2442} INFO -  at 104.0s,\testimator catboost's best error=0.1624,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 19:56:52] {2258} INFO - iteration 46, current learner lgbm\n",
      "[flaml.automl.logger: 11-13 19:56:53] {2442} INFO -  at 104.5s,\testimator lgbm's best error=0.1622,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 19:56:53] {2258} INFO - iteration 47, current learner catboost\n",
      "[flaml.automl.logger: 11-13 19:56:58] {2442} INFO -  at 109.9s,\testimator catboost's best error=0.1624,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 19:56:58] {2258} INFO - iteration 48, current learner lrl1\n",
      "[flaml.automl.logger: 11-13 19:56:58] {2442} INFO -  at 110.2s,\testimator lrl1's best error=0.1800,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 19:56:58] {2258} INFO - iteration 49, current learner sgd\n",
      "[flaml.automl.logger: 11-13 19:56:59] {2442} INFO -  at 110.3s,\testimator sgd's best error=0.1724,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 19:56:59] {2685} INFO - retrain lgbm for 0.1s\n",
      "[flaml.automl.logger: 11-13 19:56:59] {2688} INFO - retrained model: LGBMClassifier(colsample_bytree=0.8304072431299575,\n",
      "               learning_rate=0.7590459488450945, max_bin=255,\n",
      "               min_child_samples=5, n_estimators=9, n_jobs=-1, num_leaves=5,\n",
      "               reg_alpha=0.001951378031519758, reg_lambda=0.04792552866398477,\n",
      "               verbose=-1)\n",
      "[flaml.automl.logger: 11-13 19:56:59] {1985} INFO - fit succeeded\n",
      "[flaml.automl.logger: 11-13 19:56:59] {1986} INFO - Time taken to find the best model: 2.000725269317627\n",
      "0.7150259067357513\n"
     ]
    }
   ],
   "source": [
    "automl = AutoML()\n",
    "automl.fit(X_train, y_train, max_iter=50, train_time_limit=3, pred_time_limit=1e-3)\n",
    "y_hat = automl.predict(X_test)\n",
    "eval_metric= f1_score(y_test, y_hat, pos_label=\"1\")\n",
    "print(eval_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last way is to put constraints on the metrics of the ML model tried in AutoML. When users provide a custom metric function, which returns a primary optimization metric and a dictionary of additional metrics (typically also about the model) to log, users can also specify constraints on one or more of the metrics in the dictionary of additional metrics. Users need to provide a list of such constraints in the following format: Each element in this list is a 3-tuple, which shall be expressed in the following format: the first element of the 3-tuple is the name of the metric, the second element is the inequality sign chosen from \">=\" and \"\\<=\", and the third element is the constraint value. E.g., ('val_loss', '<=', 0.1). Here is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.automl.logger: 11-13 19:56:59] {1728} INFO - task = classification\n",
      "[flaml.automl.logger: 11-13 19:56:59] {1739} INFO - Evaluation method: cv\n",
      "[flaml.automl.logger: 11-13 19:56:59] {1838} INFO - Minimizing error metric: 1-roc_auc\n",
      "[flaml.automl.logger: 11-13 19:56:59] {1955} INFO - List of ML learners in AutoML Run: ['lgbm', 'rf', 'xgboost', 'extra_tree', 'xgb_limitdepth', 'sgd', 'catboost', 'lrl1']\n",
      "[flaml.automl.logger: 11-13 19:56:59] {2258} INFO - iteration 0, current learner lgbm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.automl.logger: 11-13 19:56:59] {2393} INFO - Estimated sufficient time budget=10000s. Estimated necessary time budget=10s.\n",
      "[flaml.automl.logger: 11-13 19:56:59] {2442} INFO -  at 0.2s,\testimator lgbm's best error=0.1834,\tbest estimator lgbm's best error=0.1834\n",
      "[flaml.automl.logger: 11-13 19:56:59] {2258} INFO - iteration 1, current learner lgbm\n",
      "[flaml.automl.logger: 11-13 19:56:59] {2442} INFO -  at 0.4s,\testimator lgbm's best error=0.1834,\tbest estimator lgbm's best error=0.1834\n",
      "[flaml.automl.logger: 11-13 19:56:59] {2258} INFO - iteration 2, current learner lgbm\n",
      "[flaml.automl.logger: 11-13 19:56:59] {2442} INFO -  at 0.7s,\testimator lgbm's best error=0.1769,\tbest estimator lgbm's best error=0.1769\n",
      "[flaml.automl.logger: 11-13 19:56:59] {2258} INFO - iteration 3, current learner rf\n",
      "[flaml.automl.logger: 11-13 19:57:00] {2442} INFO -  at 1.2s,\testimator rf's best error=0.1910,\tbest estimator lgbm's best error=0.1769\n",
      "[flaml.automl.logger: 11-13 19:57:00] {2258} INFO - iteration 4, current learner lgbm\n",
      "[flaml.automl.logger: 11-13 19:57:00] {2442} INFO -  at 1.5s,\testimator lgbm's best error=0.1769,\tbest estimator lgbm's best error=0.1769\n",
      "[flaml.automl.logger: 11-13 19:57:00] {2258} INFO - iteration 5, current learner lgbm\n",
      "[flaml.automl.logger: 11-13 19:57:00] {2442} INFO -  at 1.7s,\testimator lgbm's best error=0.1769,\tbest estimator lgbm's best error=0.1769\n",
      "[flaml.automl.logger: 11-13 19:57:00] {2258} INFO - iteration 6, current learner lgbm\n",
      "[flaml.automl.logger: 11-13 19:57:01] {2442} INFO -  at 2.0s,\testimator lgbm's best error=0.1769,\tbest estimator lgbm's best error=0.1769\n",
      "[flaml.automl.logger: 11-13 19:57:01] {2258} INFO - iteration 7, current learner lgbm\n",
      "[flaml.automl.logger: 11-13 19:57:01] {2442} INFO -  at 2.3s,\testimator lgbm's best error=0.1747,\tbest estimator lgbm's best error=0.1747\n",
      "[flaml.automl.logger: 11-13 19:57:01] {2258} INFO - iteration 8, current learner lgbm\n",
      "[flaml.automl.logger: 11-13 19:57:01] {2442} INFO -  at 2.6s,\testimator lgbm's best error=0.1737,\tbest estimator lgbm's best error=0.1737\n",
      "[flaml.automl.logger: 11-13 19:57:01] {2258} INFO - iteration 9, current learner lgbm\n",
      "[flaml.automl.logger: 11-13 19:57:01] {2442} INFO -  at 2.7s,\testimator lgbm's best error=0.1737,\tbest estimator lgbm's best error=0.1737\n",
      "[flaml.automl.logger: 11-13 19:57:01] {2258} INFO - iteration 10, current learner lgbm\n",
      "[flaml.automl.logger: 11-13 19:57:02] {2442} INFO -  at 3.0s,\testimator lgbm's best error=0.1737,\tbest estimator lgbm's best error=0.1737\n",
      "[flaml.automl.logger: 11-13 19:57:02] {2258} INFO - iteration 11, current learner rf\n",
      "[flaml.automl.logger: 11-13 19:57:02] {2442} INFO -  at 3.6s,\testimator rf's best error=0.1860,\tbest estimator lgbm's best error=0.1737\n",
      "[flaml.automl.logger: 11-13 19:57:02] {2258} INFO - iteration 12, current learner xgboost\n",
      "[flaml.automl.logger: 11-13 19:57:03] {2442} INFO -  at 3.9s,\testimator xgboost's best error=0.1834,\tbest estimator lgbm's best error=0.1737\n",
      "[flaml.automl.logger: 11-13 19:57:03] {2258} INFO - iteration 13, current learner xgboost\n",
      "[flaml.automl.logger: 11-13 19:57:03] {2442} INFO -  at 4.1s,\testimator xgboost's best error=0.1834,\tbest estimator lgbm's best error=0.1737\n",
      "[flaml.automl.logger: 11-13 19:57:03] {2258} INFO - iteration 14, current learner xgboost\n",
      "[flaml.automl.logger: 11-13 19:57:03] {2442} INFO -  at 4.5s,\testimator xgboost's best error=0.1834,\tbest estimator lgbm's best error=0.1737\n",
      "[flaml.automl.logger: 11-13 19:57:03] {2258} INFO - iteration 15, current learner extra_tree\n",
      "[flaml.automl.logger: 11-13 19:57:04] {2442} INFO -  at 4.9s,\testimator extra_tree's best error=0.1874,\tbest estimator lgbm's best error=0.1737\n",
      "[flaml.automl.logger: 11-13 19:57:04] {2258} INFO - iteration 16, current learner lgbm\n",
      "[flaml.automl.logger: 11-13 19:57:04] {2442} INFO -  at 5.1s,\testimator lgbm's best error=0.1696,\tbest estimator lgbm's best error=0.1696\n",
      "[flaml.automl.logger: 11-13 19:57:04] {2258} INFO - iteration 17, current learner extra_tree\n",
      "[flaml.automl.logger: 11-13 19:57:04] {2442} INFO -  at 5.6s,\testimator extra_tree's best error=0.1795,\tbest estimator lgbm's best error=0.1696\n",
      "[flaml.automl.logger: 11-13 19:57:04] {2258} INFO - iteration 18, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-13 19:57:05] {2442} INFO -  at 6.1s,\testimator xgb_limitdepth's best error=0.1707,\tbest estimator lgbm's best error=0.1696\n",
      "[flaml.automl.logger: 11-13 19:57:05] {2258} INFO - iteration 19, current learner lgbm\n",
      "[flaml.automl.logger: 11-13 19:57:05] {2442} INFO -  at 6.4s,\testimator lgbm's best error=0.1696,\tbest estimator lgbm's best error=0.1696\n",
      "[flaml.automl.logger: 11-13 19:57:05] {2258} INFO - iteration 20, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-13 19:57:05] {2442} INFO -  at 6.6s,\testimator xgb_limitdepth's best error=0.1707,\tbest estimator lgbm's best error=0.1696\n",
      "[flaml.automl.logger: 11-13 19:57:05] {2258} INFO - iteration 21, current learner extra_tree\n",
      "[flaml.automl.logger: 11-13 19:57:06] {2442} INFO -  at 7.2s,\testimator extra_tree's best error=0.1795,\tbest estimator lgbm's best error=0.1696\n",
      "[flaml.automl.logger: 11-13 19:57:06] {2258} INFO - iteration 22, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-13 19:57:06] {2442} INFO -  at 7.6s,\testimator xgb_limitdepth's best error=0.1707,\tbest estimator lgbm's best error=0.1696\n",
      "[flaml.automl.logger: 11-13 19:57:06] {2258} INFO - iteration 23, current learner sgd\n",
      "[flaml.automl.logger: 11-13 19:57:07] {2442} INFO -  at 7.9s,\testimator sgd's best error=0.1940,\tbest estimator lgbm's best error=0.1696\n",
      "[flaml.automl.logger: 11-13 19:57:07] {2258} INFO - iteration 24, current learner rf\n",
      "[flaml.automl.logger: 11-13 19:57:07] {2442} INFO -  at 8.7s,\testimator rf's best error=0.1860,\tbest estimator lgbm's best error=0.1696\n",
      "[flaml.automl.logger: 11-13 19:57:07] {2258} INFO - iteration 25, current learner xgboost\n",
      "[flaml.automl.logger: 11-13 19:57:08] {2442} INFO -  at 9.0s,\testimator xgboost's best error=0.1834,\tbest estimator lgbm's best error=0.1696\n",
      "[flaml.automl.logger: 11-13 19:57:08] {2258} INFO - iteration 26, current learner sgd\n",
      "[flaml.automl.logger: 11-13 19:57:08] {2442} INFO -  at 9.1s,\testimator sgd's best error=0.1667,\tbest estimator sgd's best error=0.1667\n",
      "[flaml.automl.logger: 11-13 19:57:08] {2258} INFO - iteration 27, current learner sgd\n",
      "[flaml.automl.logger: 11-13 19:57:08] {2442} INFO -  at 9.2s,\testimator sgd's best error=0.1667,\tbest estimator sgd's best error=0.1667\n",
      "[flaml.automl.logger: 11-13 19:57:08] {2258} INFO - iteration 28, current learner extra_tree\n",
      "[flaml.automl.logger: 11-13 19:57:08] {2442} INFO -  at 9.7s,\testimator extra_tree's best error=0.1795,\tbest estimator sgd's best error=0.1667\n",
      "[flaml.automl.logger: 11-13 19:57:08] {2258} INFO - iteration 29, current learner sgd\n",
      "[flaml.automl.logger: 11-13 19:57:09] {2442} INFO -  at 9.9s,\testimator sgd's best error=0.1667,\tbest estimator sgd's best error=0.1667\n",
      "[flaml.automl.logger: 11-13 19:57:09] {2258} INFO - iteration 30, current learner catboost\n",
      "[flaml.automl.logger: 11-13 19:57:12] {2442} INFO -  at 13.4s,\testimator catboost's best error=0.1665,\tbest estimator catboost's best error=0.1665\n",
      "[flaml.automl.logger: 11-13 19:57:12] {2258} INFO - iteration 31, current learner catboost\n",
      "[flaml.automl.logger: 11-13 19:57:45] {2442} INFO -  at 46.0s,\testimator catboost's best error=0.1633,\tbest estimator catboost's best error=0.1633\n",
      "[flaml.automl.logger: 11-13 19:57:45] {2258} INFO - iteration 32, current learner catboost\n",
      "[flaml.automl.logger: 11-13 19:57:51] {2442} INFO -  at 51.8s,\testimator catboost's best error=0.1633,\tbest estimator catboost's best error=0.1633\n",
      "[flaml.automl.logger: 11-13 19:57:51] {2258} INFO - iteration 33, current learner xgboost\n",
      "[flaml.automl.logger: 11-13 19:57:54] {2442} INFO -  at 54.8s,\testimator xgboost's best error=0.1834,\tbest estimator catboost's best error=0.1633\n",
      "[flaml.automl.logger: 11-13 19:57:54] {2258} INFO - iteration 34, current learner rf\n",
      "[flaml.automl.logger: 11-13 19:57:56] {2442} INFO -  at 57.2s,\testimator rf's best error=0.1860,\tbest estimator catboost's best error=0.1633\n",
      "[flaml.automl.logger: 11-13 19:57:56] {2258} INFO - iteration 35, current learner extra_tree\n",
      "[flaml.automl.logger: 11-13 19:57:56] {2442} INFO -  at 57.7s,\testimator extra_tree's best error=0.1795,\tbest estimator catboost's best error=0.1633\n",
      "[flaml.automl.logger: 11-13 19:57:56] {2258} INFO - iteration 36, current learner catboost\n",
      "[flaml.automl.logger: 11-13 19:58:08] {2442} INFO -  at 69.7s,\testimator catboost's best error=0.1633,\tbest estimator catboost's best error=0.1633\n",
      "[flaml.automl.logger: 11-13 19:58:08] {2258} INFO - iteration 37, current learner lrl1\n",
      "[flaml.automl.logger: 11-13 19:58:09] {2442} INFO -  at 70.0s,\testimator lrl1's best error=0.1803,\tbest estimator catboost's best error=0.1633\n",
      "[flaml.automl.logger: 11-13 19:58:09] {2258} INFO - iteration 38, current learner xgboost\n",
      "[flaml.automl.logger: 11-13 19:58:09] {2442} INFO -  at 70.4s,\testimator xgboost's best error=0.1834,\tbest estimator catboost's best error=0.1633\n",
      "[flaml.automl.logger: 11-13 19:58:09] {2258} INFO - iteration 39, current learner sgd\n",
      "[flaml.automl.logger: 11-13 19:58:09] {2442} INFO -  at 70.5s,\testimator sgd's best error=0.1667,\tbest estimator catboost's best error=0.1633\n",
      "[flaml.automl.logger: 11-13 19:58:09] {2258} INFO - iteration 40, current learner lrl1\n",
      "[flaml.automl.logger: 11-13 19:58:09] {2442} INFO -  at 70.7s,\testimator lrl1's best error=0.1803,\tbest estimator catboost's best error=0.1633\n",
      "[flaml.automl.logger: 11-13 19:58:09] {2258} INFO - iteration 41, current learner sgd\n",
      "[flaml.automl.logger: 11-13 19:58:12] {2442} INFO -  at 73.0s,\testimator sgd's best error=0.1667,\tbest estimator catboost's best error=0.1633\n",
      "[flaml.automl.logger: 11-13 19:58:12] {2258} INFO - iteration 42, current learner sgd\n",
      "[flaml.automl.logger: 11-13 19:58:14] {2442} INFO -  at 74.9s,\testimator sgd's best error=0.1667,\tbest estimator catboost's best error=0.1633\n",
      "[flaml.automl.logger: 11-13 19:58:14] {2258} INFO - iteration 43, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-13 19:58:19] {2442} INFO -  at 80.8s,\testimator xgb_limitdepth's best error=0.1707,\tbest estimator catboost's best error=0.1633\n",
      "[flaml.automl.logger: 11-13 19:58:19] {2258} INFO - iteration 44, current learner catboost\n",
      "[flaml.automl.logger: 11-13 19:58:29] {2442} INFO -  at 90.6s,\testimator catboost's best error=0.1631,\tbest estimator catboost's best error=0.1631\n",
      "[flaml.automl.logger: 11-13 19:58:29] {2258} INFO - iteration 45, current learner catboost\n",
      "[flaml.automl.logger: 11-13 19:58:33] {2442} INFO -  at 94.3s,\testimator catboost's best error=0.1631,\tbest estimator catboost's best error=0.1631\n",
      "[flaml.automl.logger: 11-13 19:58:33] {2258} INFO - iteration 46, current learner lgbm\n",
      "[flaml.automl.logger: 11-13 19:58:33] {2442} INFO -  at 94.5s,\testimator lgbm's best error=0.1696,\tbest estimator catboost's best error=0.1631\n",
      "[flaml.automl.logger: 11-13 19:58:33] {2258} INFO - iteration 47, current learner catboost\n",
      "[flaml.automl.logger: 11-13 19:58:48] {2442} INFO -  at 108.9s,\testimator catboost's best error=0.1631,\tbest estimator catboost's best error=0.1631\n",
      "[flaml.automl.logger: 11-13 19:58:48] {2258} INFO - iteration 48, current learner catboost\n",
      "[flaml.automl.logger: 11-13 19:58:51] {2442} INFO -  at 112.6s,\testimator catboost's best error=0.1631,\tbest estimator catboost's best error=0.1631\n",
      "[flaml.automl.logger: 11-13 19:58:51] {2258} INFO - iteration 49, current learner catboost\n",
      "[flaml.automl.logger: 11-13 19:59:38] {2442} INFO -  at 158.9s,\testimator catboost's best error=0.1629,\tbest estimator catboost's best error=0.1629\n",
      "[flaml.automl.logger: 11-13 19:59:51] {2685} INFO - retrain catboost for 13.4s\n",
      "[flaml.automl.logger: 11-13 19:59:51] {2688} INFO - retrained model: <catboost.core.CatBoostClassifier object at 0x000001E76D46FF70>\n",
      "[flaml.automl.logger: 11-13 19:59:51] {1985} INFO - fit succeeded\n",
      "[flaml.automl.logger: 11-13 19:59:51] {1986} INFO - Time taken to find the best model: 158.9410698413849\n",
      "0.7015706806282721\n"
     ]
    }
   ],
   "source": [
    "def custom_metric(X_val, y_val, estimator, labels, X_train, y_train, weight_val=None, weight_train=None, *args):\n",
    "\n",
    "    \"\"\"\n",
    "    Custom metric function to evaluate an estimator's performance.\n",
    "\n",
    "    This function calculates the validation loss and training loss using log loss, \n",
    "    and returns a weighted combination of these losses along with additional metrics.\n",
    "\n",
    "    Parameters:\n",
    "    X_val (array-like): Validation feature set.\n",
    "    y_val (array-like): Validation target values.\n",
    "    estimator (object): The estimator to evaluate.\n",
    "    labels (array-like): List of labels for classification.\n",
    "    X_train (array-like): Training feature set.\n",
    "    y_train (array-like): Training target values.\n",
    "    weight_val (array-like, optional): Sample weights for validation set.\n",
    "    weight_train (array-like, optional): Sample weights for training set.\n",
    "    *args: Additional (optional) arguments.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing the weighted loss and a dictionary with validation loss, \n",
    "           training loss, and prediction time per sample.\n",
    "    \"\"\"\n",
    "\n",
    "    start = time.time()\n",
    "    y_pred = estimator.predict_proba(X_val)\n",
    "    pred_time = (time.time() - start) / len(X_val)\n",
    "    val_loss = log_loss(y_val, y_pred, labels=labels, sample_weight=weight_val)\n",
    "    y_pred = estimator.predict_proba(X_train)\n",
    "    train_loss = log_loss(y_train, y_pred, labels=labels, sample_weight=weight_train)\n",
    "    alpha = 0.5\n",
    "    return val_loss * (1 + alpha) - alpha * train_loss, {\n",
    "        \"val_loss\": val_loss,\n",
    "        \"train_loss\": train_loss,\n",
    "        \"pred_time\": pred_time,\n",
    "    }\n",
    "\n",
    "metric_constraints = [(\"train_loss\", \"<=\", 0.1), (\"val_loss\", \"<=\", 0.1)]\n",
    "automl.fit(X_train, y_train, max_iter=50, train_time_limit=1, metric_constraints=metric_constraints)\n",
    "y_hat = automl.predict(X_test)\n",
    "eval_metric= f1_score(y_test, y_hat, pos_label=\"1\")\n",
    "print(eval_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ensemble\n",
    "\n",
    "To use stacked ensemble after the model search, set _ensemble_=True or a dict in the fit() method. When  _ensemble_=True, the final estimator and passthrough in the stacker will be automatically chosen. You can specify customized final estimator or passthrough option:\n",
    "\n",
    "   - \"final_estimator\": an instance of the final estimator in the stacker (usually a logit model will suffice);\n",
    "   - \"passthrough\": True (default) or False, whether to pass the original features to the stacker.\n",
    "\n",
    "The following example will run for a while (roughly an hour) if we don't use time budget:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.automl.logger: 11-13 19:59:51] {1728} INFO - task = classification\n",
      "[flaml.automl.logger: 11-13 19:59:51] {1739} INFO - Evaluation method: cv\n",
      "[flaml.automl.logger: 11-13 19:59:51] {1838} INFO - Minimizing error metric: 1-roc_auc\n",
      "[flaml.automl.logger: 11-13 19:59:51] {1861} WARNING - No search budget is provided via time_budget or max_iter. Training only one model per estimator. Zero-shot AutoML is used for certain tasks and estimators. To tune hyperparameters for each estimator, please provide budget either via time_budget or max_iter.\n",
      "[flaml.automl.logger: 11-13 19:59:51] {1955} INFO - List of ML learners in AutoML Run: ['rf', 'lgbm', 'xgboost', 'extra_tree', 'xgb_limitdepth', 'sgd', 'catboost', 'lrl1']\n",
      "[flaml.automl.logger: 11-13 19:59:51] {2258} INFO - iteration 0, current learner rf\n",
      "[flaml.automl.logger: 11-13 19:59:59] {2393} INFO - Estimated sufficient time budget=10000s. Estimated necessary time budget=10s.\n",
      "[flaml.automl.logger: 11-13 19:59:59] {2442} INFO -  at 8.0s,\testimator rf's best error=0.1779,\tbest estimator rf's best error=0.1779\n",
      "[flaml.automl.logger: 11-13 19:59:59] {2258} INFO - iteration 1, current learner lgbm\n",
      "[flaml.automl.logger: 11-13 20:00:12] {2442} INFO -  at 20.5s,\testimator lgbm's best error=0.1691,\tbest estimator lgbm's best error=0.1691\n",
      "[flaml.automl.logger: 11-13 20:00:12] {2258} INFO - iteration 2, current learner xgboost\n",
      "[flaml.automl.logger: 11-13 21:06:12] {2442} INFO -  at 3980.5s,\testimator xgboost's best error=0.1755,\tbest estimator lgbm's best error=0.1691\n",
      "[flaml.automl.logger: 11-13 21:06:12] {2258} INFO - iteration 3, current learner extra_tree\n",
      "[flaml.automl.logger: 11-13 21:06:16] {2442} INFO -  at 3984.9s,\testimator extra_tree's best error=0.1801,\tbest estimator lgbm's best error=0.1691\n",
      "[flaml.automl.logger: 11-13 21:06:16] {2258} INFO - iteration 4, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-13 21:06:49] {2442} INFO -  at 4017.9s,\testimator xgb_limitdepth's best error=0.1717,\tbest estimator lgbm's best error=0.1691\n",
      "[flaml.automl.logger: 11-13 21:06:49] {2258} INFO - iteration 5, current learner sgd\n",
      "[flaml.automl.logger: 11-13 21:06:49] {2442} INFO -  at 4018.1s,\testimator sgd's best error=0.1942,\tbest estimator lgbm's best error=0.1691\n",
      "[flaml.automl.logger: 11-13 21:06:49] {2258} INFO - iteration 6, current learner catboost\n",
      "[flaml.automl.logger: 11-13 21:06:52] {2442} INFO -  at 4020.8s,\testimator catboost's best error=0.1665,\tbest estimator catboost's best error=0.1665\n",
      "[flaml.automl.logger: 11-13 21:06:52] {2258} INFO - iteration 7, current learner lrl1\n",
      "[flaml.automl.logger: 11-13 21:06:52] {2442} INFO -  at 4020.9s,\testimator lrl1's best error=0.1803,\tbest estimator catboost's best error=0.1665\n",
      "[flaml.automl.logger: 11-13 21:06:52] {2582} INFO - [('catboost', {'early_stopping_rounds': 10, 'learning_rate': 0.09999999999999996, 'n_estimators': 8192, 'thread_count': -1, 'verbose': False, 'random_seed': 10242048}), ('lgbm', {'n_jobs': -1, 'n_estimators': 362, 'num_leaves': 1208, 'min_child_samples': 8, 'learning_rate': 0.02070742242160566, 'colsample_bytree': 0.37915528071680865, 'reg_alpha': 0.002982599447751338, 'reg_lambda': 1.136605174453919, 'max_bin': 15, 'verbose': -1}), ('xgb_limitdepth', {'n_jobs': -1, 'n_estimators': 877, 'max_depth': 11, 'min_child_weight': 0.6205465771093738, 'learning_rate': 0.013622118381700795, 'subsample': 0.566692814245426, 'colsample_bylevel': 0.8865741642101924, 'colsample_bytree': 1.0, 'reg_alpha': 0.01386336444764391, 'reg_lambda': 3.113947886074155, 'verbosity': 0}), ('xgboost', {'n_jobs': -1, 'n_estimators': 13499, 'max_leaves': 60, 'min_child_weight': 0.008494221584011285, 'learning_rate': 0.006955765856675575, 'subsample': 0.5965241023754743, 'colsample_bylevel': 0.590641168068946, 'colsample_bytree': 1.0, 'reg_alpha': 0.2522240954379289, 'reg_lambda': 5.351809144038808, 'max_depth': 0, 'grow_policy': 'lossguide', 'tree_method': 'hist', 'verbosity': 0}), ('rf', {'n_jobs': -1, 'n_estimators': 356, 'max_features': 0.1, 'criterion': 'gini', 'max_leaf_nodes': 102, 'random_state': 12032022, 'verbose': 0}), ('extra_tree', {'n_jobs': -1, 'n_estimators': 408, 'max_features': 0.3629795757973624, 'criterion': 'entropy', 'max_leaf_nodes': 81, 'random_state': 12032022, 'verbose': 0}), ('lrl1', {'n_jobs': -1, 'C': 1.0, 'tol': 0.0001, 'solver': 'saga', 'penalty': 'l1'}), ('sgd', {'n_jobs': -1, 'penalty': 'l2', 'alpha': 0.0001, 'epsilon': 0.1, 'learning_rate': 'invscaling', 'eta0': 0.010000000000000005, 'power_t': 0.5, 'average': False, 'loss': 'modified_huber', 'tol': 0.0001})]\n",
      "[flaml.automl.logger: 11-13 21:06:52] {2625} INFO - Building ensemble with tuned estimators\n",
      "[flaml.automl.logger: 11-13 21:15:30] {2631} INFO - ensemble: StackingClassifier(estimators=[('catboost',\n",
      "                                <flaml.automl.model.CatBoostEstimator object at 0x000001E769285780>),\n",
      "                               ('lgbm',\n",
      "                                <flaml.automl.model.LGBMEstimator object at 0x000001E769285F60>),\n",
      "                               ('xgb_limitdepth',\n",
      "                                <flaml.automl.model.XGBoostLimitDepthEstimator object at 0x000001E769284AF0>),\n",
      "                               ('xgboost',\n",
      "                                <flaml.automl.model.XGBoostSklearnEstimator object at 0x000001E769269A20>),\n",
      "                               ('rf',\n",
      "                                <flaml.automl.model.RandomForestEstimator object at 0x000001E76926AF20>),\n",
      "                               ('extra_tree',\n",
      "                                <flaml.automl.model.ExtraTreesEstimator object at 0x000001E76926A3B0>),\n",
      "                               ('lrl1',\n",
      "                                <flaml.automl.model.LRL1Classifier object at 0x000001E76926AB90>),\n",
      "                               ('sgd',\n",
      "                                <flaml.automl.model.SGDEstimator object at 0x000001E76926BAC0>)],\n",
      "                   final_estimator=LogisticRegression(), n_jobs=1)\n",
      "[flaml.automl.logger: 11-13 21:15:30] {1985} INFO - fit succeeded\n",
      "[flaml.automl.logger: 11-13 21:15:30] {1986} INFO - Time taken to find the best model: 4020.7610437870026\n",
      "0.7272727272727274\n"
     ]
    }
   ],
   "source": [
    "titanic = fetch_openml('titanic', version=1, as_frame=True)\n",
    "data = titanic.frame\n",
    "features = [\"pclass\", \"sex\", \"sibsp\", \"parch\", \"embarked\"]\n",
    "target = \"survived\"\n",
    "X = data[features]\n",
    "y = data[target]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "X_train = pd.get_dummies(X_train[features])\n",
    "X_test  = pd.get_dummies(X_test[features])\n",
    "\n",
    "automl.fit(X_train, y_train, task=\"classification\", time_budget=360, ensemble={\"final_estimator\": LogisticRegression(), \"passthrough\": False})\n",
    "y_hat = automl.predict(X_test)\n",
    "eval_metric = f1_score(y_test, y_hat, pos_label=\"1\")\n",
    "print(eval_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resampling Strategy\n",
    "\n",
    "By default, FLAML decides the resampling automatically according to the data size and the time budget. If you would like to enforce a certain resampling strategy, you can set _eva_method_ to be \"holdout\" or \"cv\" for holdout or cross-validation in the fit() method. \n",
    "\n",
    "For holdout, you can also set:\n",
    "\n",
    "   - _split\\_ratio_: the fraction for validation data, 0.1 by default;\n",
    "   - _X\\_val_, _y\\_val_: a separate validation dataset. When they are passed, the validation metrics will be computed against this given validation dataset. If they are not passed, then a validation dataset will be split from the training data and held out from training during the model search. After the model search, FLAML will retrain the model with best configuration on the full training data. You can set _retrain\\_full_ to be false to skip the final retraining or \"budget\" to ask FLAML to do its best to retrain within the time budget. When  _retrain\\_full_ is set to  be true, the user-provided validation data is not used in the final retraining of the model.\n",
    "\n",
    "For cross validation, you can also set _n\\_splits_ of the number of folds. By default it is 5.\n",
    "\n",
    "FLAML relies on the provided task type to infer the default splitting strategy:\n",
    "\n",
    "   - stratified split for classification;\n",
    "   - uniform split for regression;\n",
    "   - time-based split for time series forecasting;\n",
    "   - group-based split for learning to rank.\n",
    "\n",
    "The data split method for classification can be changed into uniform split by setting _split\\_type_=\"uniform\". The data are shuffled when _split\\_type_ in (\"uniform\", \"stratified\"). For both classification and regression tasks more advanced split configurations are possible:\n",
    "\n",
    "   - time-based split can be enforced if the data are sorted by timestamps, by setting _split\\_type_=\"time\";\n",
    "   - group-based splits can be set by using _split\\_type_=\"group\" while providing the group identifier for each sample through the groups argument.\n",
    "\n",
    "Below let's see an example. The code begins by setting up a random number generator with a fixed seed to ensure reproducibility. It then defines two color maps, `cmap_data` and `cmap_cv`, which will be used for visualizing the data and cross-validation splits, respectively. Next, the code generates a synthetic dataset. It creates a matrix of shape (100, 10) with random values drawn from a standard normal distribution. The target variable `y` is generated as a binary array of length 100, where each element is randomly assigned a value of 0 or 1. This is achieved by generating random values between 0 and 1 and converting those greater than 0.5 to 1 and the rest to 0. The code also creates uneven groups within the dataset. It uses a Dirichlet distribution to generate a `group_prior` array of length 10, which represents the proportions of each group. The `groups` array is then created by repeating the group indices according to the proportions specified in `group_prior`, resulting in an array of length 100. We then visualize the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "100\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rng = np.random.RandomState(1338)\n",
    "cmap_data = plt.cm.Paired\n",
    "cmap_cv = plt.cm.coolwarm\n",
    "\n",
    "n_points = 100\n",
    "X = rng.randn(100, 10) # 100 by 10\n",
    "\n",
    "np.random.seed(2023)\n",
    "y = (np.random.rand(n_points) > 0.5).astype(int)  # modified to avoid groups having uniform label, len(y)=100 in this case\n",
    "\n",
    "group_prior = rng.dirichlet([2] * 10) #generating uneven groups\n",
    "groups = np.repeat(np.arange(10), rng.multinomial(100, group_prior))\n",
    "\n",
    "print(len(group_prior)) # 10\n",
    "print(len(groups)) # 100\n",
    "\n",
    "def visualize_groups(classes, groups, name):\n",
    "    # Visualize dataset groups\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.scatter(\n",
    "        range(len(groups)),\n",
    "        [0.5] * len(groups),\n",
    "        c=groups,\n",
    "        marker=\"_\",\n",
    "        lw=50,\n",
    "        cmap=cmap_data,\n",
    "    )\n",
    "    ax.scatter(\n",
    "        range(len(groups)),\n",
    "        [3.5] * len(groups),\n",
    "        c=classes,\n",
    "        marker=\"_\",\n",
    "        lw=50,\n",
    "        cmap=cmap_data,\n",
    "    )\n",
    "    ax.set(\n",
    "        ylim=[-1, 5],\n",
    "        yticks=[0.5, 3.5],\n",
    "        yticklabels=[\"Data\\ngroup\", \"Data\\nclass\"],\n",
    "        xlabel=\"Sample index\",\n",
    "    )\n",
    "\n",
    "visualize_groups(y, groups, \"no groups\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's further create another function to faciliate visualization. We then can run FLAML, evaluating the results on a cross-validation, without setting groups first. This applies the default split settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cv_indices(cv, X, y, group, ax, n_splits, lw=10):\n",
    "    \"\"\"Create a sample plot for indices of a cross-validation object.\n",
    "       Function source: https://scikit-learn.org/stable/auto_examples/model_selection/plot_cv_indices.html\n",
    "    \"\"\"\n",
    "\n",
    "    for ii, (tr, tt) in enumerate(cv.split(X=X, y=y, groups=group)): # generating the training/test visualizations for each CV split\n",
    "        indices = np.array([np.nan] * len(X)) # filling in indices with training/test groups\n",
    "        indices[tt] = 1\n",
    "        indices[tr] = 0\n",
    "\n",
    "        ax.scatter(\n",
    "            range(len(indices)),\n",
    "            [ii + 0.5] * len(indices),\n",
    "            c=indices,\n",
    "            marker=\"_\",\n",
    "            lw=lw,\n",
    "            cmap=cmap_cv,\n",
    "            vmin=-0.2,\n",
    "            vmax=1.2,\n",
    "        )\n",
    "\n",
    "    ax.scatter(\n",
    "        range(len(X)), [ii + 1.5] * len(X), c=y, marker=\"_\", lw=lw, cmap=cmap_data\n",
    "    ) # plotting the data classes and groups at the end\n",
    "    ax.scatter(\n",
    "        range(len(X)), [ii + 2.5] * len(X), c=group, marker=\"_\", lw=lw, cmap=cmap_data\n",
    "    ) # plotting the data classes and groups at the end\n",
    "\n",
    "    yticklabels = list(range(n_splits)) + [\"class\", \"group\"] # formatting\n",
    "    ax.set(\n",
    "        yticks=np.arange(n_splits + 2) + 0.5,\n",
    "        yticklabels=yticklabels,\n",
    "        xlabel=\"Sample index\",\n",
    "        ylabel=\"CV iteration\",\n",
    "        ylim=[n_splits + 2.2, -0.2],\n",
    "        xlim=[0, 100],\n",
    "    )\n",
    "    ax.set_title(\"{}\".format(type(cv).__name__), fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.automl.logger: 11-13 21:15:30] {1728} INFO - task = classification\n",
      "[flaml.automl.logger: 11-13 21:15:30] {1739} INFO - Evaluation method: cv\n",
      "[flaml.automl.logger: 11-13 21:15:30] {1838} INFO - Minimizing error metric: 1-accuracy\n",
      "[flaml.automl.logger: 11-13 21:15:30] {1955} INFO - List of ML learners in AutoML Run: ['rf', 'kneighbor', 'xgboost']\n",
      "[flaml.automl.logger: 11-13 21:15:30] {2258} INFO - iteration 0, current learner rf\n",
      "[flaml.automl.logger: 11-13 21:15:30] {2393} INFO - Estimated sufficient time budget=4740s. Estimated necessary time budget=8s.\n",
      "[flaml.automl.logger: 11-13 21:15:30] {2442} INFO -  at 0.5s,\testimator rf's best error=0.4697,\tbest estimator rf's best error=0.4697\n",
      "[flaml.automl.logger: 11-13 21:15:30] {2258} INFO - iteration 1, current learner xgboost\n",
      "[flaml.automl.logger: 11-13 21:15:31] {2442} INFO -  at 0.6s,\testimator xgboost's best error=0.4305,\tbest estimator xgboost's best error=0.4305\n",
      "[flaml.automl.logger: 11-13 21:15:31] {2258} INFO - iteration 2, current learner xgboost\n",
      "[flaml.automl.logger: 11-13 21:15:31] {2442} INFO -  at 0.7s,\testimator xgboost's best error=0.4305,\tbest estimator xgboost's best error=0.4305\n",
      "[flaml.automl.logger: 11-13 21:15:31] {2258} INFO - iteration 3, current learner xgboost\n",
      "[flaml.automl.logger: 11-13 21:15:31] {2442} INFO -  at 0.7s,\testimator xgboost's best error=0.4302,\tbest estimator xgboost's best error=0.4302\n",
      "[flaml.automl.logger: 11-13 21:15:31] {2258} INFO - iteration 4, current learner rf\n",
      "[flaml.automl.logger: 11-13 21:15:31] {2442} INFO -  at 1.1s,\testimator rf's best error=0.4697,\tbest estimator xgboost's best error=0.4302\n",
      "[flaml.automl.logger: 11-13 21:15:31] {2258} INFO - iteration 5, current learner xgboost\n",
      "[flaml.automl.logger: 11-13 21:15:31] {2442} INFO -  at 1.2s,\testimator xgboost's best error=0.4302,\tbest estimator xgboost's best error=0.4302\n",
      "[flaml.automl.logger: 11-13 21:15:31] {2258} INFO - iteration 6, current learner rf\n",
      "[flaml.automl.logger: 11-13 21:15:31] {2442} INFO -  at 1.5s,\testimator rf's best error=0.4697,\tbest estimator xgboost's best error=0.4302\n",
      "[flaml.automl.logger: 11-13 21:15:31] {2258} INFO - iteration 7, current learner rf\n",
      "[flaml.automl.logger: 11-13 21:15:32] {2442} INFO -  at 1.9s,\testimator rf's best error=0.4599,\tbest estimator xgboost's best error=0.4302\n",
      "[flaml.automl.logger: 11-13 21:15:32] {2258} INFO - iteration 8, current learner xgboost\n",
      "[flaml.automl.logger: 11-13 21:15:32] {2442} INFO -  at 2.0s,\testimator xgboost's best error=0.4302,\tbest estimator xgboost's best error=0.4302\n",
      "[flaml.automl.logger: 11-13 21:15:32] {2258} INFO - iteration 9, current learner xgboost\n",
      "[flaml.automl.logger: 11-13 21:15:32] {2442} INFO -  at 2.1s,\testimator xgboost's best error=0.3904,\tbest estimator xgboost's best error=0.3904\n",
      "[flaml.automl.logger: 11-13 21:15:32] {2258} INFO - iteration 10, current learner xgboost\n",
      "[flaml.automl.logger: 11-13 21:15:32] {2442} INFO -  at 2.2s,\testimator xgboost's best error=0.3904,\tbest estimator xgboost's best error=0.3904\n",
      "[flaml.automl.logger: 11-13 21:15:32] {2258} INFO - iteration 11, current learner xgboost\n",
      "[flaml.automl.logger: 11-13 21:15:32] {2442} INFO -  at 2.3s,\testimator xgboost's best error=0.3904,\tbest estimator xgboost's best error=0.3904\n",
      "[flaml.automl.logger: 11-13 21:15:32] {2258} INFO - iteration 12, current learner xgboost\n",
      "[flaml.automl.logger: 11-13 21:15:32] {2442} INFO -  at 2.4s,\testimator xgboost's best error=0.3904,\tbest estimator xgboost's best error=0.3904\n",
      "[flaml.automl.logger: 11-13 21:15:32] {2258} INFO - iteration 13, current learner xgboost\n",
      "[flaml.automl.logger: 11-13 21:15:32] {2442} INFO -  at 2.5s,\testimator xgboost's best error=0.3904,\tbest estimator xgboost's best error=0.3904\n",
      "[flaml.automl.logger: 11-13 21:15:32] {2258} INFO - iteration 14, current learner xgboost\n",
      "[flaml.automl.logger: 11-13 21:15:33] {2442} INFO -  at 2.6s,\testimator xgboost's best error=0.3904,\tbest estimator xgboost's best error=0.3904\n",
      "[flaml.automl.logger: 11-13 21:15:33] {2258} INFO - iteration 15, current learner xgboost\n",
      "[flaml.automl.logger: 11-13 21:15:33] {2442} INFO -  at 2.7s,\testimator xgboost's best error=0.3904,\tbest estimator xgboost's best error=0.3904\n",
      "[flaml.automl.logger: 11-13 21:15:33] {2258} INFO - iteration 16, current learner xgboost\n",
      "[flaml.automl.logger: 11-13 21:15:33] {2442} INFO -  at 2.8s,\testimator xgboost's best error=0.3904,\tbest estimator xgboost's best error=0.3904\n",
      "[flaml.automl.logger: 11-13 21:15:33] {2258} INFO - iteration 17, current learner xgboost\n",
      "[flaml.automl.logger: 11-13 21:15:33] {2442} INFO -  at 2.9s,\testimator xgboost's best error=0.3904,\tbest estimator xgboost's best error=0.3904\n",
      "[flaml.automl.logger: 11-13 21:15:33] {2258} INFO - iteration 18, current learner xgboost\n",
      "[flaml.automl.logger: 11-13 21:15:33] {2442} INFO -  at 3.0s,\testimator xgboost's best error=0.3904,\tbest estimator xgboost's best error=0.3904\n",
      "[flaml.automl.logger: 11-13 21:15:33] {2685} INFO - retrain xgboost for 0.0s\n",
      "[flaml.automl.logger: 11-13 21:15:33] {2688} INFO - retrained model: XGBClassifier(base_score=None, booster=None, callbacks=[],\n",
      "              colsample_bylevel=0.8184166881476597, colsample_bynode=None,\n",
      "              colsample_bytree=0.8648827061331837, device=None,\n",
      "              early_stopping_rounds=None, enable_categorical=False,\n",
      "              eval_metric=None, feature_types=None, gamma=None,\n",
      "              grow_policy='lossguide', importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.1374664252308776,\n",
      "              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=0, max_leaves=4,\n",
      "              min_child_weight=0.08262716617929555, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=4,\n",
      "              n_jobs=-1, num_parallel_tree=None, random_state=None, ...)\n",
      "[flaml.automl.logger: 11-13 21:15:33] {1985} INFO - fit succeeded\n",
      "[flaml.automl.logger: 11-13 21:15:33] {1986} INFO - Time taken to find the best model: 2.0881292819976807\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "automl = AutoML()\n",
    "automl_settings = {\n",
    "    \"time_budget\": 3,  # total running time in seconds\n",
    "    \"metric\": 'accuracy', \n",
    "    \"estimator_list\": [\"rf\", \"kneighbor\", \"xgboost\"],\n",
    "    \"task\": 'classification',  # task type    \n",
    "    \"log_file_name\": 'undestanding_cross_validation_default.log',\n",
    "    \"log_training_metric\": True,  # whether to log training metric\n",
    "    \"keep_search_state\": True, # needed if you want to keep the cross validation information\n",
    "    \"eval_method\": \"cv\",\n",
    "    #\"split_type\": \"group\",\n",
    "    #\"groups\": groups,\n",
    "    \"n_splits\": 3\n",
    "}\n",
    "\n",
    "automl.fit(X, y, **automl_settings)\n",
    "\n",
    "f, ax = plt.subplots(1,1)\n",
    "plot_cv_indices(automl._state.kf, X, y, groups, ax, automl._state.kf.get_n_splits())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's set the split type to groups and provide the groups to run a GroupKFold instead. Compare with the previous visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.automl.logger: 11-13 21:15:33] {1728} INFO - task = classification\n",
      "[flaml.automl.logger: 11-13 21:15:33] {1739} INFO - Evaluation method: cv\n",
      "[flaml.automl.logger: 11-13 21:15:33] {1838} INFO - Minimizing error metric: 1-accuracy\n",
      "[flaml.automl.logger: 11-13 21:15:33] {1955} INFO - List of ML learners in AutoML Run: ['rf', 'kneighbor', 'xgboost']\n",
      "[flaml.automl.logger: 11-13 21:15:33] {2258} INFO - iteration 0, current learner rf\n",
      "[flaml.automl.logger: 11-13 21:15:34] {2393} INFO - Estimated sufficient time budget=3440s. Estimated necessary time budget=6s.\n",
      "[flaml.automl.logger: 11-13 21:15:34] {2442} INFO -  at 0.4s,\testimator rf's best error=0.4798,\tbest estimator rf's best error=0.4798\n",
      "[flaml.automl.logger: 11-13 21:15:34] {2258} INFO - iteration 1, current learner xgboost\n",
      "[flaml.automl.logger: 11-13 21:15:34] {2442} INFO -  at 0.5s,\testimator xgboost's best error=0.4191,\tbest estimator xgboost's best error=0.4191\n",
      "[flaml.automl.logger: 11-13 21:15:34] {2258} INFO - iteration 2, current learner xgboost\n",
      "[flaml.automl.logger: 11-13 21:15:34] {2442} INFO -  at 0.5s,\testimator xgboost's best error=0.4191,\tbest estimator xgboost's best error=0.4191\n",
      "[flaml.automl.logger: 11-13 21:15:34] {2258} INFO - iteration 3, current learner xgboost\n",
      "[flaml.automl.logger: 11-13 21:15:34] {2442} INFO -  at 0.6s,\testimator xgboost's best error=0.4191,\tbest estimator xgboost's best error=0.4191\n",
      "[flaml.automl.logger: 11-13 21:15:34] {2258} INFO - iteration 4, current learner rf\n",
      "[flaml.automl.logger: 11-13 21:15:34] {2442} INFO -  at 1.0s,\testimator rf's best error=0.4798,\tbest estimator xgboost's best error=0.4191\n",
      "[flaml.automl.logger: 11-13 21:15:34] {2258} INFO - iteration 5, current learner rf\n",
      "[flaml.automl.logger: 11-13 21:15:35] {2442} INFO -  at 1.4s,\testimator rf's best error=0.4798,\tbest estimator xgboost's best error=0.4191\n",
      "[flaml.automl.logger: 11-13 21:15:35] {2258} INFO - iteration 6, current learner rf\n",
      "[flaml.automl.logger: 11-13 21:15:35] {2442} INFO -  at 1.8s,\testimator rf's best error=0.4706,\tbest estimator xgboost's best error=0.4191\n",
      "[flaml.automl.logger: 11-13 21:15:35] {2258} INFO - iteration 7, current learner xgboost\n",
      "[flaml.automl.logger: 11-13 21:15:35] {2442} INFO -  at 1.9s,\testimator xgboost's best error=0.4191,\tbest estimator xgboost's best error=0.4191\n",
      "[flaml.automl.logger: 11-13 21:15:35] {2258} INFO - iteration 8, current learner xgboost\n",
      "[flaml.automl.logger: 11-13 21:15:35] {2442} INFO -  at 2.0s,\testimator xgboost's best error=0.4191,\tbest estimator xgboost's best error=0.4191\n",
      "[flaml.automl.logger: 11-13 21:15:35] {2258} INFO - iteration 9, current learner xgboost\n",
      "[flaml.automl.logger: 11-13 21:15:35] {2442} INFO -  at 2.1s,\testimator xgboost's best error=0.4191,\tbest estimator xgboost's best error=0.4191\n",
      "[flaml.automl.logger: 11-13 21:15:35] {2258} INFO - iteration 10, current learner xgboost\n",
      "[flaml.automl.logger: 11-13 21:15:35] {2442} INFO -  at 2.2s,\testimator xgboost's best error=0.4191,\tbest estimator xgboost's best error=0.4191\n",
      "[flaml.automl.logger: 11-13 21:15:35] {2258} INFO - iteration 11, current learner xgboost\n",
      "[flaml.automl.logger: 11-13 21:15:36] {2442} INFO -  at 2.3s,\testimator xgboost's best error=0.4191,\tbest estimator xgboost's best error=0.4191\n",
      "[flaml.automl.logger: 11-13 21:15:36] {2258} INFO - iteration 12, current learner xgboost\n",
      "[flaml.automl.logger: 11-13 21:15:36] {2442} INFO -  at 2.4s,\testimator xgboost's best error=0.4191,\tbest estimator xgboost's best error=0.4191\n",
      "[flaml.automl.logger: 11-13 21:15:36] {2258} INFO - iteration 13, current learner xgboost\n",
      "[flaml.automl.logger: 11-13 21:15:36] {2442} INFO -  at 2.4s,\testimator xgboost's best error=0.4191,\tbest estimator xgboost's best error=0.4191\n",
      "[flaml.automl.logger: 11-13 21:15:36] {2258} INFO - iteration 14, current learner xgboost\n",
      "[flaml.automl.logger: 11-13 21:15:36] {2442} INFO -  at 2.5s,\testimator xgboost's best error=0.4191,\tbest estimator xgboost's best error=0.4191\n",
      "[flaml.automl.logger: 11-13 21:15:36] {2258} INFO - iteration 15, current learner xgboost\n",
      "[flaml.automl.logger: 11-13 21:15:36] {2442} INFO -  at 2.6s,\testimator xgboost's best error=0.4191,\tbest estimator xgboost's best error=0.4191\n",
      "[flaml.automl.logger: 11-13 21:15:36] {2258} INFO - iteration 16, current learner xgboost\n",
      "[flaml.automl.logger: 11-13 21:15:36] {2442} INFO -  at 2.7s,\testimator xgboost's best error=0.4191,\tbest estimator xgboost's best error=0.4191\n",
      "[flaml.automl.logger: 11-13 21:15:36] {2258} INFO - iteration 17, current learner xgboost\n",
      "[flaml.automl.logger: 11-13 21:15:36] {2442} INFO -  at 2.8s,\testimator xgboost's best error=0.4191,\tbest estimator xgboost's best error=0.4191\n",
      "[flaml.automl.logger: 11-13 21:15:36] {2258} INFO - iteration 18, current learner xgboost\n",
      "[flaml.automl.logger: 11-13 21:15:36] {2442} INFO -  at 2.9s,\testimator xgboost's best error=0.4191,\tbest estimator xgboost's best error=0.4191\n",
      "[flaml.automl.logger: 11-13 21:15:36] {2258} INFO - iteration 19, current learner kneighbor\n",
      "[flaml.automl.logger: 11-13 21:15:36] {2442} INFO -  at 3.1s,\testimator kneighbor's best error=0.5398,\tbest estimator xgboost's best error=0.4191\n",
      "[flaml.automl.logger: 11-13 21:15:36] {2685} INFO - retrain xgboost for 0.0s\n",
      "[flaml.automl.logger: 11-13 21:15:36] {2688} INFO - retrained model: XGBClassifier(base_score=None, booster=None, callbacks=[],\n",
      "              colsample_bylevel=1.0, colsample_bynode=None,\n",
      "              colsample_bytree=1.0, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy='lossguide', importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.09999999999999995,\n",
      "              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=0, max_leaves=4,\n",
      "              min_child_weight=0.9999999999999993, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=4,\n",
      "              n_jobs=-1, num_parallel_tree=None, random_state=None, ...)\n",
      "[flaml.automl.logger: 11-13 21:15:36] {1985} INFO - fit succeeded\n",
      "[flaml.automl.logger: 11-13 21:15:36] {1986} INFO - Time taken to find the best model: 0.4704623222351074\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "automl_settings[\"split_type\"] = \"group\"\n",
    "automl_settings[\"groups\"] = groups\n",
    "automl_settings[\"log_file_name\"] = 'undestanding_cross_validation_groupkfold.log'\n",
    "\n",
    "automl = AutoML()\n",
    "automl.fit(X, y, **automl_settings)\n",
    "\n",
    "f, ax = plt.subplots(1,1)\n",
    "plot_cv_indices(automl._state.kf, X, y, groups, ax, automl._state.kf.get_n_splits())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parallel Tuning and Warm Start\n",
    "\n",
    "When you have parallel resources, you can either spend them in training and keep the model search sequential, or perform parallel search. You can either use the `ray` package or Spark to do parallel tuning. The documentation has more details. \n",
    "\n",
    "We can warm start the AutoML by providing starting points of hyperparameter configurstions for each estimator. For example, if you have run AutoML for one hour, after checking the results, you would like to run it for another two hours, then you can use the best configurations found for each estimator as the starting points for the new run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.automl.logger: 11-13 21:15:37] {1728} INFO - task = classification\n",
      "[flaml.automl.logger: 11-13 21:15:37] {1739} INFO - Evaluation method: cv\n",
      "[flaml.automl.logger: 11-13 21:15:37] {1838} INFO - Minimizing error metric: 1-roc_auc\n",
      "[flaml.automl.logger: 11-13 21:15:37] {1955} INFO - List of ML learners in AutoML Run: ['lgbm', 'rf', 'xgboost', 'extra_tree', 'xgb_limitdepth', 'sgd', 'catboost', 'lrl1']\n",
      "[flaml.automl.logger: 11-13 21:15:37] {2258} INFO - iteration 0, current learner lgbm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PGAO\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\datasets\\_openml.py:1002: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.automl.logger: 11-13 21:15:37] {2393} INFO - Estimated sufficient time budget=1323s. Estimated necessary time budget=33s.\n",
      "[flaml.automl.logger: 11-13 21:15:37] {2442} INFO -  at 0.1s,\testimator lgbm's best error=0.1834,\tbest estimator lgbm's best error=0.1834\n",
      "[flaml.automl.logger: 11-13 21:15:37] {2258} INFO - iteration 1, current learner lgbm\n",
      "[flaml.automl.logger: 11-13 21:15:37] {2442} INFO -  at 0.3s,\testimator lgbm's best error=0.1834,\tbest estimator lgbm's best error=0.1834\n",
      "[flaml.automl.logger: 11-13 21:15:37] {2258} INFO - iteration 2, current learner lgbm\n",
      "[flaml.automl.logger: 11-13 21:15:37] {2442} INFO -  at 0.4s,\testimator lgbm's best error=0.1812,\tbest estimator lgbm's best error=0.1812\n",
      "[flaml.automl.logger: 11-13 21:15:37] {2258} INFO - iteration 3, current learner sgd\n",
      "[flaml.automl.logger: 11-13 21:15:37] {2442} INFO -  at 0.6s,\testimator sgd's best error=0.1941,\tbest estimator lgbm's best error=0.1812\n",
      "[flaml.automl.logger: 11-13 21:15:37] {2258} INFO - iteration 4, current learner lgbm\n",
      "[flaml.automl.logger: 11-13 21:15:37] {2442} INFO -  at 0.8s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
      "[flaml.automl.logger: 11-13 21:15:37] {2258} INFO - iteration 5, current learner lgbm\n",
      "[flaml.automl.logger: 11-13 21:15:38] {2442} INFO -  at 0.9s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
      "[flaml.automl.logger: 11-13 21:15:38] {2258} INFO - iteration 6, current learner lgbm\n",
      "[flaml.automl.logger: 11-13 21:15:38] {2442} INFO -  at 1.2s,\testimator lgbm's best error=0.1622,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 21:15:38] {2258} INFO - iteration 7, current learner lgbm\n",
      "[flaml.automl.logger: 11-13 21:15:38] {2442} INFO -  at 1.4s,\testimator lgbm's best error=0.1622,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 21:15:38] {2258} INFO - iteration 8, current learner lgbm\n",
      "[flaml.automl.logger: 11-13 21:15:38] {2442} INFO -  at 1.6s,\testimator lgbm's best error=0.1622,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 21:15:38] {2258} INFO - iteration 9, current learner sgd\n",
      "[flaml.automl.logger: 11-13 21:15:38] {2442} INFO -  at 1.7s,\testimator sgd's best error=0.1747,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 21:15:38] {2258} INFO - iteration 10, current learner sgd\n",
      "[flaml.automl.logger: 11-13 21:15:38] {2442} INFO -  at 1.8s,\testimator sgd's best error=0.1747,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 21:15:38] {2258} INFO - iteration 11, current learner xgboost\n",
      "[flaml.automl.logger: 11-13 21:15:39] {2442} INFO -  at 1.9s,\testimator xgboost's best error=0.1834,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 21:15:39] {2258} INFO - iteration 12, current learner xgboost\n",
      "[flaml.automl.logger: 11-13 21:15:39] {2442} INFO -  at 2.3s,\testimator xgboost's best error=0.1828,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 21:15:39] {2258} INFO - iteration 13, current learner sgd\n",
      "[flaml.automl.logger: 11-13 21:15:39] {2442} INFO -  at 2.4s,\testimator sgd's best error=0.1747,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 21:15:39] {2258} INFO - iteration 14, current learner extra_tree\n",
      "[flaml.automl.logger: 11-13 21:15:39] {2442} INFO -  at 2.8s,\testimator extra_tree's best error=0.1874,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 21:15:39] {2258} INFO - iteration 15, current learner rf\n",
      "[flaml.automl.logger: 11-13 21:15:40] {2442} INFO -  at 3.1s,\testimator rf's best error=0.1910,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 21:15:40] {2258} INFO - iteration 16, current learner extra_tree\n",
      "[flaml.automl.logger: 11-13 21:15:40] {2442} INFO -  at 3.4s,\testimator extra_tree's best error=0.1824,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 21:15:40] {2258} INFO - iteration 17, current learner lgbm\n",
      "[flaml.automl.logger: 11-13 21:15:40] {2442} INFO -  at 3.9s,\testimator lgbm's best error=0.1622,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 21:15:40] {2258} INFO - iteration 18, current learner sgd\n",
      "[flaml.automl.logger: 11-13 21:15:41] {2442} INFO -  at 3.9s,\testimator sgd's best error=0.1747,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 21:15:41] {2258} INFO - iteration 19, current learner lgbm\n",
      "[flaml.automl.logger: 11-13 21:15:41] {2442} INFO -  at 4.0s,\testimator lgbm's best error=0.1622,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 21:15:41] {2258} INFO - iteration 20, current learner sgd\n",
      "[flaml.automl.logger: 11-13 21:15:41] {2442} INFO -  at 4.1s,\testimator sgd's best error=0.1725,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 21:15:41] {2258} INFO - iteration 21, current learner rf\n",
      "[flaml.automl.logger: 11-13 21:15:41] {2442} INFO -  at 4.6s,\testimator rf's best error=0.1910,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 21:15:41] {2258} INFO - iteration 22, current learner rf\n",
      "[flaml.automl.logger: 11-13 21:15:42] {2442} INFO -  at 5.0s,\testimator rf's best error=0.1910,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 21:15:42] {2258} INFO - iteration 23, current learner lgbm\n",
      "[flaml.automl.logger: 11-13 21:15:42] {2442} INFO -  at 5.5s,\testimator lgbm's best error=0.1622,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 21:15:42] {2258} INFO - iteration 24, current learner lgbm\n",
      "[flaml.automl.logger: 11-13 21:15:42] {2442} INFO -  at 5.8s,\testimator lgbm's best error=0.1622,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 21:15:42] {2258} INFO - iteration 25, current learner extra_tree\n",
      "[flaml.automl.logger: 11-13 21:15:43] {2442} INFO -  at 6.1s,\testimator extra_tree's best error=0.1824,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 21:15:43] {2258} INFO - iteration 26, current learner sgd\n",
      "[flaml.automl.logger: 11-13 21:15:43] {2442} INFO -  at 6.2s,\testimator sgd's best error=0.1716,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 21:15:43] {2258} INFO - iteration 27, current learner rf\n",
      "[flaml.automl.logger: 11-13 21:15:43] {2442} INFO -  at 6.6s,\testimator rf's best error=0.1910,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 21:15:43] {2258} INFO - iteration 28, current learner sgd\n",
      "[flaml.automl.logger: 11-13 21:15:43] {2442} INFO -  at 6.7s,\testimator sgd's best error=0.1716,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 21:15:43] {2258} INFO - iteration 29, current learner sgd\n",
      "[flaml.automl.logger: 11-13 21:15:43] {2442} INFO -  at 6.7s,\testimator sgd's best error=0.1716,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 21:15:43] {2258} INFO - iteration 30, current learner sgd\n",
      "[flaml.automl.logger: 11-13 21:15:43] {2442} INFO -  at 6.8s,\testimator sgd's best error=0.1716,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 21:15:43] {2258} INFO - iteration 31, current learner lgbm\n",
      "[flaml.automl.logger: 11-13 21:15:44] {2442} INFO -  at 7.0s,\testimator lgbm's best error=0.1622,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 21:15:44] {2258} INFO - iteration 32, current learner lgbm\n",
      "[flaml.automl.logger: 11-13 21:15:44] {2442} INFO -  at 7.3s,\testimator lgbm's best error=0.1622,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 21:15:44] {2258} INFO - iteration 33, current learner lgbm\n",
      "[flaml.automl.logger: 11-13 21:15:44] {2442} INFO -  at 7.5s,\testimator lgbm's best error=0.1622,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 21:15:44] {2258} INFO - iteration 34, current learner sgd\n",
      "[flaml.automl.logger: 11-13 21:15:44] {2442} INFO -  at 7.6s,\testimator sgd's best error=0.1716,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 21:15:44] {2258} INFO - iteration 35, current learner lgbm\n",
      "[flaml.automl.logger: 11-13 21:15:44] {2442} INFO -  at 7.8s,\testimator lgbm's best error=0.1622,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 21:15:44] {2258} INFO - iteration 36, current learner rf\n",
      "[flaml.automl.logger: 11-13 21:15:45] {2442} INFO -  at 8.2s,\testimator rf's best error=0.1857,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 21:15:45] {2258} INFO - iteration 37, current learner catboost\n",
      "[flaml.automl.logger: 11-13 21:15:47] {2442} INFO -  at 10.2s,\testimator catboost's best error=0.1665,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 21:15:47] {2258} INFO - iteration 38, current learner catboost\n",
      "[flaml.automl.logger: 11-13 21:15:49] {2442} INFO -  at 12.6s,\testimator catboost's best error=0.1631,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 21:15:49] {2258} INFO - iteration 39, current learner extra_tree\n",
      "[flaml.automl.logger: 11-13 21:15:50] {2442} INFO -  at 13.3s,\testimator extra_tree's best error=0.1824,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 21:15:50] {2258} INFO - iteration 40, current learner catboost\n",
      "[flaml.automl.logger: 11-13 21:15:52] {2442} INFO -  at 15.4s,\testimator catboost's best error=0.1631,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 21:15:52] {2258} INFO - iteration 41, current learner lgbm\n",
      "[flaml.automl.logger: 11-13 21:15:52] {2442} INFO -  at 15.7s,\testimator lgbm's best error=0.1622,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 21:15:52] {2258} INFO - iteration 42, current learner catboost\n",
      "[flaml.automl.logger: 11-13 21:15:56] {2442} INFO -  at 19.4s,\testimator catboost's best error=0.1624,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 21:15:56] {2258} INFO - iteration 43, current learner catboost\n",
      "[flaml.automl.logger: 11-13 21:15:58] {2442} INFO -  at 21.9s,\testimator catboost's best error=0.1624,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 21:15:58] {2258} INFO - iteration 44, current learner lgbm\n",
      "[flaml.automl.logger: 11-13 21:15:59] {2442} INFO -  at 22.1s,\testimator lgbm's best error=0.1622,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 21:15:59] {2258} INFO - iteration 45, current learner sgd\n",
      "[flaml.automl.logger: 11-13 21:15:59] {2442} INFO -  at 22.1s,\testimator sgd's best error=0.1716,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 21:15:59] {2258} INFO - iteration 46, current learner catboost\n",
      "[flaml.automl.logger: 11-13 21:16:04] {2442} INFO -  at 27.5s,\testimator catboost's best error=0.1624,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 21:16:04] {2258} INFO - iteration 47, current learner sgd\n",
      "[flaml.automl.logger: 11-13 21:16:04] {2442} INFO -  at 27.6s,\testimator sgd's best error=0.1716,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 21:16:04] {2258} INFO - iteration 48, current learner lgbm\n",
      "[flaml.automl.logger: 11-13 21:16:05] {2442} INFO -  at 27.9s,\testimator lgbm's best error=0.1622,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 21:16:05] {2258} INFO - iteration 49, current learner sgd\n",
      "[flaml.automl.logger: 11-13 21:16:05] {2442} INFO -  at 28.0s,\testimator sgd's best error=0.1716,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 21:16:05] {2258} INFO - iteration 50, current learner lgbm\n",
      "[flaml.automl.logger: 11-13 21:16:05] {2442} INFO -  at 28.2s,\testimator lgbm's best error=0.1622,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 21:16:05] {2258} INFO - iteration 51, current learner rf\n",
      "[flaml.automl.logger: 11-13 21:16:06] {2442} INFO -  at 28.9s,\testimator rf's best error=0.1800,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 21:16:06] {2258} INFO - iteration 52, current learner catboost\n",
      "[flaml.automl.logger: 11-13 21:16:08] {2442} INFO -  at 31.4s,\testimator catboost's best error=0.1624,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 21:16:08] {2258} INFO - iteration 53, current learner lgbm\n",
      "[flaml.automl.logger: 11-13 21:16:09] {2442} INFO -  at 31.9s,\testimator lgbm's best error=0.1622,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 21:16:09] {2258} INFO - iteration 54, current learner lgbm\n",
      "[flaml.automl.logger: 11-13 21:16:09] {2442} INFO -  at 32.3s,\testimator lgbm's best error=0.1622,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 21:16:09] {2258} INFO - iteration 55, current learner lgbm\n",
      "[flaml.automl.logger: 11-13 21:16:09] {2442} INFO -  at 32.5s,\testimator lgbm's best error=0.1622,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 21:16:09] {2258} INFO - iteration 56, current learner lgbm\n",
      "[flaml.automl.logger: 11-13 21:16:09] {2442} INFO -  at 32.8s,\testimator lgbm's best error=0.1622,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 21:16:09] {2258} INFO - iteration 57, current learner extra_tree\n",
      "[flaml.automl.logger: 11-13 21:16:10] {2442} INFO -  at 33.1s,\testimator extra_tree's best error=0.1684,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 21:16:10] {2258} INFO - iteration 58, current learner rf\n",
      "[flaml.automl.logger: 11-13 21:16:10] {2442} INFO -  at 33.6s,\testimator rf's best error=0.1800,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 21:16:10] {2258} INFO - iteration 59, current learner xgboost\n",
      "[flaml.automl.logger: 11-13 21:16:10] {2442} INFO -  at 33.8s,\testimator xgboost's best error=0.1820,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 21:16:10] {2258} INFO - iteration 60, current learner extra_tree\n",
      "[flaml.automl.logger: 11-13 21:16:11] {2442} INFO -  at 34.3s,\testimator extra_tree's best error=0.1684,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 21:16:11] {2258} INFO - iteration 61, current learner lgbm\n",
      "[flaml.automl.logger: 11-13 21:16:11] {2442} INFO -  at 34.6s,\testimator lgbm's best error=0.1622,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 21:16:11] {2258} INFO - iteration 62, current learner extra_tree\n",
      "[flaml.automl.logger: 11-13 21:16:12] {2442} INFO -  at 35.0s,\testimator extra_tree's best error=0.1684,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 21:16:12] {2258} INFO - iteration 63, current learner lgbm\n",
      "[flaml.automl.logger: 11-13 21:16:12] {2442} INFO -  at 35.2s,\testimator lgbm's best error=0.1622,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 21:16:12] {2258} INFO - iteration 64, current learner extra_tree\n",
      "[flaml.automl.logger: 11-13 21:16:12] {2442} INFO -  at 35.7s,\testimator extra_tree's best error=0.1684,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 21:16:12] {2258} INFO - iteration 65, current learner extra_tree\n",
      "[flaml.automl.logger: 11-13 21:16:13] {2442} INFO -  at 36.1s,\testimator extra_tree's best error=0.1684,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 21:16:13] {2258} INFO - iteration 66, current learner lgbm\n",
      "[flaml.automl.logger: 11-13 21:16:13] {2442} INFO -  at 36.5s,\testimator lgbm's best error=0.1622,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 21:16:13] {2258} INFO - iteration 67, current learner xgboost\n",
      "[flaml.automl.logger: 11-13 21:16:13] {2442} INFO -  at 36.7s,\testimator xgboost's best error=0.1801,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 21:16:13] {2258} INFO - iteration 68, current learner extra_tree\n",
      "[flaml.automl.logger: 11-13 21:16:14] {2442} INFO -  at 37.0s,\testimator extra_tree's best error=0.1684,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 21:16:14] {2258} INFO - iteration 69, current learner xgboost\n",
      "[flaml.automl.logger: 11-13 21:16:14] {2442} INFO -  at 37.2s,\testimator xgboost's best error=0.1801,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 21:16:14] {2258} INFO - iteration 70, current learner lgbm\n",
      "[flaml.automl.logger: 11-13 21:16:14] {2442} INFO -  at 37.7s,\testimator lgbm's best error=0.1622,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 21:16:14] {2258} INFO - iteration 71, current learner extra_tree\n",
      "[flaml.automl.logger: 11-13 21:16:15] {2442} INFO -  at 38.1s,\testimator extra_tree's best error=0.1677,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 21:16:15] {2258} INFO - iteration 72, current learner xgboost\n",
      "[flaml.automl.logger: 11-13 21:16:15] {2442} INFO -  at 38.3s,\testimator xgboost's best error=0.1791,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 21:16:15] {2258} INFO - iteration 73, current learner xgboost\n",
      "[flaml.automl.logger: 11-13 21:16:15] {2442} INFO -  at 38.6s,\testimator xgboost's best error=0.1703,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 21:16:15] {2258} INFO - iteration 74, current learner sgd\n",
      "[flaml.automl.logger: 11-13 21:16:15] {2442} INFO -  at 38.7s,\testimator sgd's best error=0.1716,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 21:16:15] {2258} INFO - iteration 75, current learner xgboost\n",
      "[flaml.automl.logger: 11-13 21:16:16] {2442} INFO -  at 39.0s,\testimator xgboost's best error=0.1651,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 21:16:16] {2258} INFO - iteration 76, current learner xgboost\n",
      "[flaml.automl.logger: 11-13 21:16:16] {2442} INFO -  at 39.3s,\testimator xgboost's best error=0.1651,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 21:16:16] {2258} INFO - iteration 77, current learner rf\n",
      "[flaml.automl.logger: 11-13 21:16:17] {2442} INFO -  at 40.1s,\testimator rf's best error=0.1713,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 21:16:17] {2258} INFO - iteration 78, current learner rf\n",
      "[flaml.automl.logger: 11-13 21:16:17] {2442} INFO -  at 40.7s,\testimator rf's best error=0.1713,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 21:16:17] {2258} INFO - iteration 79, current learner xgboost\n",
      "[flaml.automl.logger: 11-13 21:16:18] {2442} INFO -  at 41.1s,\testimator xgboost's best error=0.1651,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 21:16:18] {2258} INFO - iteration 80, current learner xgboost\n",
      "[flaml.automl.logger: 11-13 21:16:18] {2442} INFO -  at 41.5s,\testimator xgboost's best error=0.1640,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 21:16:18] {2258} INFO - iteration 81, current learner lgbm\n",
      "[flaml.automl.logger: 11-13 21:16:18] {2442} INFO -  at 41.6s,\testimator lgbm's best error=0.1622,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 21:16:18] {2258} INFO - iteration 82, current learner catboost\n",
      "[flaml.automl.logger: 11-13 21:16:22] {2442} INFO -  at 45.6s,\testimator catboost's best error=0.1624,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 21:16:22] {2258} INFO - iteration 83, current learner rf\n",
      "[flaml.automl.logger: 11-13 21:16:23] {2442} INFO -  at 46.1s,\testimator rf's best error=0.1713,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 21:16:23] {2258} INFO - iteration 84, current learner xgboost\n",
      "[flaml.automl.logger: 11-13 21:16:23] {2442} INFO -  at 46.3s,\testimator xgboost's best error=0.1640,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 21:16:23] {2258} INFO - iteration 85, current learner xgboost\n",
      "[flaml.automl.logger: 11-13 21:16:23] {2442} INFO -  at 46.8s,\testimator xgboost's best error=0.1640,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 21:16:23] {2258} INFO - iteration 86, current learner sgd\n",
      "[flaml.automl.logger: 11-13 21:16:24] {2442} INFO -  at 46.9s,\testimator sgd's best error=0.1716,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 21:16:24] {2258} INFO - iteration 87, current learner xgboost\n",
      "[flaml.automl.logger: 11-13 21:16:24] {2442} INFO -  at 47.1s,\testimator xgboost's best error=0.1640,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 21:16:24] {2258} INFO - iteration 88, current learner extra_tree\n",
      "[flaml.automl.logger: 11-13 21:16:24] {2442} INFO -  at 47.6s,\testimator extra_tree's best error=0.1677,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 21:16:24] {2258} INFO - iteration 89, current learner rf\n",
      "[flaml.automl.logger: 11-13 21:16:25] {2442} INFO -  at 48.2s,\testimator rf's best error=0.1667,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 21:16:25] {2258} INFO - iteration 90, current learner rf\n",
      "[flaml.automl.logger: 11-13 21:16:25] {2442} INFO -  at 48.9s,\testimator rf's best error=0.1667,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 21:16:25] {2258} INFO - iteration 91, current learner sgd\n",
      "[flaml.automl.logger: 11-13 21:16:26] {2442} INFO -  at 48.9s,\testimator sgd's best error=0.1716,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 21:16:26] {2258} INFO - iteration 92, current learner rf\n",
      "[flaml.automl.logger: 11-13 21:16:26] {2442} INFO -  at 49.5s,\testimator rf's best error=0.1653,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 21:16:26] {2258} INFO - iteration 93, current learner catboost\n",
      "[flaml.automl.logger: 11-13 21:16:29] {2442} INFO -  at 52.2s,\testimator catboost's best error=0.1624,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 21:16:29] {2258} INFO - iteration 94, current learner xgboost\n",
      "[flaml.automl.logger: 11-13 21:16:30] {2442} INFO -  at 53.0s,\testimator xgboost's best error=0.1640,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 21:16:30] {2258} INFO - iteration 95, current learner xgboost\n",
      "[flaml.automl.logger: 11-13 21:16:30] {2442} INFO -  at 53.3s,\testimator xgboost's best error=0.1640,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 21:16:30] {2258} INFO - iteration 96, current learner lgbm\n",
      "[flaml.automl.logger: 11-13 21:16:30] {2442} INFO -  at 53.5s,\testimator lgbm's best error=0.1622,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 21:16:30] {2258} INFO - iteration 97, current learner catboost\n",
      "[flaml.automl.logger: 11-13 21:16:33] {2442} INFO -  at 56.8s,\testimator catboost's best error=0.1624,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 21:16:33] {2258} INFO - iteration 98, current learner rf\n",
      "[flaml.automl.logger: 11-13 21:16:34] {2442} INFO -  at 57.6s,\testimator rf's best error=0.1610,\tbest estimator rf's best error=0.1610\n",
      "[flaml.automl.logger: 11-13 21:16:34] {2258} INFO - iteration 99, current learner rf\n",
      "[flaml.automl.logger: 11-13 21:16:35] {2442} INFO -  at 58.1s,\testimator rf's best error=0.1610,\tbest estimator rf's best error=0.1610\n",
      "[flaml.automl.logger: 11-13 21:16:35] {2258} INFO - iteration 100, current learner rf\n",
      "[flaml.automl.logger: 11-13 21:16:35] {2442} INFO -  at 58.8s,\testimator rf's best error=0.1610,\tbest estimator rf's best error=0.1610\n",
      "[flaml.automl.logger: 11-13 21:16:35] {2258} INFO - iteration 101, current learner rf\n",
      "[flaml.automl.logger: 11-13 21:16:36] {2442} INFO -  at 59.6s,\testimator rf's best error=0.1610,\tbest estimator rf's best error=0.1610\n",
      "[flaml.automl.logger: 11-13 21:16:36] {2258} INFO - iteration 102, current learner sgd\n",
      "[flaml.automl.logger: 11-13 21:16:36] {2442} INFO -  at 59.6s,\testimator sgd's best error=0.1716,\tbest estimator rf's best error=0.1610\n",
      "[flaml.automl.logger: 11-13 21:16:36] {2258} INFO - iteration 103, current learner xgboost\n",
      "[flaml.automl.logger: 11-13 21:16:37] {2442} INFO -  at 60.0s,\testimator xgboost's best error=0.1640,\tbest estimator rf's best error=0.1610\n",
      "[flaml.automl.logger: 11-13 21:16:37] {2685} INFO - retrain rf for 0.0s\n",
      "[flaml.automl.logger: 11-13 21:16:37] {2688} INFO - retrained model: RandomForestClassifier(criterion='entropy', max_features=0.527173642134884,\n",
      "                       max_leaf_nodes=25, n_estimators=11, n_jobs=-1,\n",
      "                       random_state=12032022)\n",
      "[flaml.automl.logger: 11-13 21:16:37] {1985} INFO - fit succeeded\n",
      "[flaml.automl.logger: 11-13 21:16:37] {1986} INFO - Time taken to find the best model: 57.5848171710968\n",
      "[flaml.automl.logger: 11-13 21:16:37] {1728} INFO - task = classification\n",
      "[flaml.automl.logger: 11-13 21:16:37] {1739} INFO - Evaluation method: cv\n",
      "[flaml.automl.logger: 11-13 21:16:37] {1838} INFO - Minimizing error metric: 1-roc_auc\n",
      "[flaml.automl.logger: 11-13 21:16:37] {1955} INFO - List of ML learners in AutoML Run: ['lgbm', 'rf', 'xgboost', 'extra_tree', 'xgb_limitdepth', 'sgd', 'catboost', 'lrl1']\n",
      "[flaml.automl.logger: 11-13 21:16:37] {2258} INFO - iteration 0, current learner lgbm\n",
      "[flaml.automl.logger: 11-13 21:16:37] {2393} INFO - Estimated sufficient time budget=2605s. Estimated necessary time budget=64s.\n",
      "[flaml.automl.logger: 11-13 21:16:37] {2442} INFO -  at 0.3s,\testimator lgbm's best error=0.1622,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 21:16:37] {2258} INFO - iteration 1, current learner lgbm\n",
      "[flaml.automl.logger: 11-13 21:16:37] {2442} INFO -  at 0.5s,\testimator lgbm's best error=0.1622,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 21:16:37] {2258} INFO - iteration 2, current learner lgbm\n",
      "[flaml.automl.logger: 11-13 21:16:37] {2442} INFO -  at 0.8s,\testimator lgbm's best error=0.1622,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 21:16:37] {2258} INFO - iteration 3, current learner sgd\n",
      "[flaml.automl.logger: 11-13 21:16:38] {2442} INFO -  at 0.8s,\testimator sgd's best error=0.1712,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 21:16:38] {2258} INFO - iteration 4, current learner lgbm\n",
      "[flaml.automl.logger: 11-13 21:16:38] {2442} INFO -  at 1.3s,\testimator lgbm's best error=0.1622,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 21:16:38] {2258} INFO - iteration 5, current learner sgd\n",
      "[flaml.automl.logger: 11-13 21:16:38] {2442} INFO -  at 1.4s,\testimator sgd's best error=0.1712,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 21:16:38] {2258} INFO - iteration 6, current learner sgd\n",
      "[flaml.automl.logger: 11-13 21:16:38] {2442} INFO -  at 1.5s,\testimator sgd's best error=0.1712,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 21:16:38] {2258} INFO - iteration 7, current learner lgbm\n",
      "[flaml.automl.logger: 11-13 21:16:38] {2442} INFO -  at 1.7s,\testimator lgbm's best error=0.1622,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 21:16:38] {2258} INFO - iteration 8, current learner lgbm\n",
      "[flaml.automl.logger: 11-13 21:16:39] {2442} INFO -  at 2.0s,\testimator lgbm's best error=0.1622,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 21:16:39] {2258} INFO - iteration 9, current learner sgd\n",
      "[flaml.automl.logger: 11-13 21:16:39] {2442} INFO -  at 2.1s,\testimator sgd's best error=0.1712,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 21:16:39] {2258} INFO - iteration 10, current learner sgd\n",
      "[flaml.automl.logger: 11-13 21:16:39] {2442} INFO -  at 2.1s,\testimator sgd's best error=0.1712,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 21:16:39] {2258} INFO - iteration 11, current learner sgd\n",
      "[flaml.automl.logger: 11-13 21:16:39] {2442} INFO -  at 2.2s,\testimator sgd's best error=0.1712,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 21:16:39] {2258} INFO - iteration 12, current learner xgboost\n",
      "[flaml.automl.logger: 11-13 21:16:39] {2442} INFO -  at 2.5s,\testimator xgboost's best error=0.1640,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 21:16:39] {2258} INFO - iteration 13, current learner xgboost\n",
      "[flaml.automl.logger: 11-13 21:16:40] {2442} INFO -  at 2.9s,\testimator xgboost's best error=0.1640,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 21:16:40] {2258} INFO - iteration 14, current learner sgd\n",
      "[flaml.automl.logger: 11-13 21:16:40] {2442} INFO -  at 3.0s,\testimator sgd's best error=0.1710,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 21:16:40] {2258} INFO - iteration 15, current learner lgbm\n",
      "[flaml.automl.logger: 11-13 21:16:40] {2442} INFO -  at 3.2s,\testimator lgbm's best error=0.1622,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 21:16:40] {2258} INFO - iteration 16, current learner xgboost\n",
      "[flaml.automl.logger: 11-13 21:16:40] {2442} INFO -  at 3.6s,\testimator xgboost's best error=0.1640,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 21:16:40] {2258} INFO - iteration 17, current learner extra_tree\n",
      "[flaml.automl.logger: 11-13 21:16:41] {2442} INFO -  at 4.2s,\testimator extra_tree's best error=0.1677,\tbest estimator lgbm's best error=0.1622\n",
      "[flaml.automl.logger: 11-13 21:16:41] {2258} INFO - iteration 18, current learner rf\n",
      "[flaml.automl.logger: 11-13 21:16:42] {2442} INFO -  at 4.8s,\testimator rf's best error=0.1610,\tbest estimator rf's best error=0.1610\n",
      "[flaml.automl.logger: 11-13 21:16:42] {2258} INFO - iteration 19, current learner lgbm\n",
      "[flaml.automl.logger: 11-13 21:16:42] {2442} INFO -  at 5.0s,\testimator lgbm's best error=0.1622,\tbest estimator rf's best error=0.1610\n",
      "[flaml.automl.logger: 11-13 21:16:42] {2258} INFO - iteration 20, current learner extra_tree\n",
      "[flaml.automl.logger: 11-13 21:16:42] {2442} INFO -  at 5.4s,\testimator extra_tree's best error=0.1677,\tbest estimator rf's best error=0.1610\n",
      "[flaml.automl.logger: 11-13 21:16:42] {2258} INFO - iteration 21, current learner xgboost\n",
      "[flaml.automl.logger: 11-13 21:16:42] {2442} INFO -  at 5.7s,\testimator xgboost's best error=0.1640,\tbest estimator rf's best error=0.1610\n",
      "[flaml.automl.logger: 11-13 21:16:42] {2258} INFO - iteration 22, current learner xgboost\n",
      "[flaml.automl.logger: 11-13 21:16:43] {2442} INFO -  at 6.1s,\testimator xgboost's best error=0.1640,\tbest estimator rf's best error=0.1610\n",
      "[flaml.automl.logger: 11-13 21:16:43] {2258} INFO - iteration 23, current learner rf\n",
      "[flaml.automl.logger: 11-13 21:16:44] {2442} INFO -  at 6.8s,\testimator rf's best error=0.1610,\tbest estimator rf's best error=0.1610\n",
      "[flaml.automl.logger: 11-13 21:16:44] {2258} INFO - iteration 24, current learner rf\n",
      "[flaml.automl.logger: 11-13 21:16:44] {2442} INFO -  at 7.4s,\testimator rf's best error=0.1610,\tbest estimator rf's best error=0.1610\n",
      "[flaml.automl.logger: 11-13 21:16:44] {2258} INFO - iteration 25, current learner extra_tree\n",
      "[flaml.automl.logger: 11-13 21:16:45] {2442} INFO -  at 7.9s,\testimator extra_tree's best error=0.1677,\tbest estimator rf's best error=0.1610\n",
      "[flaml.automl.logger: 11-13 21:16:45] {2258} INFO - iteration 26, current learner extra_tree\n",
      "[flaml.automl.logger: 11-13 21:16:45] {2442} INFO -  at 8.5s,\testimator extra_tree's best error=0.1677,\tbest estimator rf's best error=0.1610\n",
      "[flaml.automl.logger: 11-13 21:16:45] {2258} INFO - iteration 27, current learner rf\n",
      "[flaml.automl.logger: 11-13 21:16:46] {2442} INFO -  at 9.3s,\testimator rf's best error=0.1610,\tbest estimator rf's best error=0.1610\n",
      "[flaml.automl.logger: 11-13 21:16:46] {2258} INFO - iteration 28, current learner xgboost\n",
      "[flaml.automl.logger: 11-13 21:16:46] {2442} INFO -  at 9.6s,\testimator xgboost's best error=0.1640,\tbest estimator rf's best error=0.1610\n",
      "[flaml.automl.logger: 11-13 21:16:46] {2258} INFO - iteration 29, current learner extra_tree\n",
      "[flaml.automl.logger: 11-13 21:16:47] {2442} INFO -  at 10.0s,\testimator extra_tree's best error=0.1677,\tbest estimator rf's best error=0.1610\n",
      "[flaml.automl.logger: 11-13 21:16:47] {2258} INFO - iteration 30, current learner extra_tree\n",
      "[flaml.automl.logger: 11-13 21:16:47] {2442} INFO -  at 10.4s,\testimator extra_tree's best error=0.1677,\tbest estimator rf's best error=0.1610\n",
      "[flaml.automl.logger: 11-13 21:16:47] {2258} INFO - iteration 31, current learner lgbm\n",
      "[flaml.automl.logger: 11-13 21:16:48] {2442} INFO -  at 10.9s,\testimator lgbm's best error=0.1622,\tbest estimator rf's best error=0.1610\n",
      "[flaml.automl.logger: 11-13 21:16:48] {2258} INFO - iteration 32, current learner lgbm\n",
      "[flaml.automl.logger: 11-13 21:16:48] {2442} INFO -  at 11.0s,\testimator lgbm's best error=0.1622,\tbest estimator rf's best error=0.1610\n",
      "[flaml.automl.logger: 11-13 21:16:48] {2258} INFO - iteration 33, current learner lgbm\n",
      "[flaml.automl.logger: 11-13 21:16:48] {2442} INFO -  at 11.5s,\testimator lgbm's best error=0.1622,\tbest estimator rf's best error=0.1610\n",
      "[flaml.automl.logger: 11-13 21:16:48] {2258} INFO - iteration 34, current learner extra_tree\n",
      "[flaml.automl.logger: 11-13 21:16:49] {2442} INFO -  at 12.1s,\testimator extra_tree's best error=0.1677,\tbest estimator rf's best error=0.1610\n",
      "[flaml.automl.logger: 11-13 21:16:49] {2258} INFO - iteration 35, current learner lgbm\n",
      "[flaml.automl.logger: 11-13 21:16:49] {2442} INFO -  at 12.4s,\testimator lgbm's best error=0.1622,\tbest estimator rf's best error=0.1610\n",
      "[flaml.automl.logger: 11-13 21:16:49] {2258} INFO - iteration 36, current learner rf\n",
      "[flaml.automl.logger: 11-13 21:16:50] {2442} INFO -  at 13.0s,\testimator rf's best error=0.1610,\tbest estimator rf's best error=0.1610\n",
      "[flaml.automl.logger: 11-13 21:16:50] {2258} INFO - iteration 37, current learner extra_tree\n",
      "[flaml.automl.logger: 11-13 21:16:50] {2442} INFO -  at 13.3s,\testimator extra_tree's best error=0.1650,\tbest estimator rf's best error=0.1610\n",
      "[flaml.automl.logger: 11-13 21:16:50] {2258} INFO - iteration 38, current learner xgboost\n",
      "[flaml.automl.logger: 11-13 21:16:51] {2442} INFO -  at 13.9s,\testimator xgboost's best error=0.1640,\tbest estimator rf's best error=0.1610\n",
      "[flaml.automl.logger: 11-13 21:16:51] {2258} INFO - iteration 39, current learner xgboost\n",
      "[flaml.automl.logger: 11-13 21:16:51] {2442} INFO -  at 14.4s,\testimator xgboost's best error=0.1640,\tbest estimator rf's best error=0.1610\n",
      "[flaml.automl.logger: 11-13 21:16:51] {2258} INFO - iteration 40, current learner rf\n",
      "[flaml.automl.logger: 11-13 21:16:52] {2442} INFO -  at 15.1s,\testimator rf's best error=0.1610,\tbest estimator rf's best error=0.1610\n",
      "[flaml.automl.logger: 11-13 21:16:52] {2258} INFO - iteration 41, current learner xgboost\n",
      "[flaml.automl.logger: 11-13 21:16:52] {2442} INFO -  at 15.4s,\testimator xgboost's best error=0.1640,\tbest estimator rf's best error=0.1610\n",
      "[flaml.automl.logger: 11-13 21:16:52] {2258} INFO - iteration 42, current learner xgboost\n",
      "[flaml.automl.logger: 11-13 21:16:52] {2442} INFO -  at 15.7s,\testimator xgboost's best error=0.1640,\tbest estimator rf's best error=0.1610\n",
      "[flaml.automl.logger: 11-13 21:16:52] {2258} INFO - iteration 43, current learner lgbm\n",
      "[flaml.automl.logger: 11-13 21:16:53] {2442} INFO -  at 15.9s,\testimator lgbm's best error=0.1622,\tbest estimator rf's best error=0.1610\n",
      "[flaml.automl.logger: 11-13 21:16:53] {2258} INFO - iteration 44, current learner xgboost\n",
      "[flaml.automl.logger: 11-13 21:16:53] {2442} INFO -  at 16.3s,\testimator xgboost's best error=0.1640,\tbest estimator rf's best error=0.1610\n",
      "[flaml.automl.logger: 11-13 21:16:53] {2258} INFO - iteration 45, current learner sgd\n",
      "[flaml.automl.logger: 11-13 21:16:53] {2442} INFO -  at 16.3s,\testimator sgd's best error=0.1710,\tbest estimator rf's best error=0.1610\n",
      "[flaml.automl.logger: 11-13 21:16:53] {2258} INFO - iteration 46, current learner xgboost\n",
      "[flaml.automl.logger: 11-13 21:16:53] {2442} INFO -  at 16.6s,\testimator xgboost's best error=0.1640,\tbest estimator rf's best error=0.1610\n",
      "[flaml.automl.logger: 11-13 21:16:53] {2258} INFO - iteration 47, current learner lgbm\n",
      "[flaml.automl.logger: 11-13 21:16:54] {2442} INFO -  at 17.0s,\testimator lgbm's best error=0.1622,\tbest estimator rf's best error=0.1610\n",
      "[flaml.automl.logger: 11-13 21:16:54] {2258} INFO - iteration 48, current learner xgboost\n",
      "[flaml.automl.logger: 11-13 21:16:54] {2442} INFO -  at 17.5s,\testimator xgboost's best error=0.1640,\tbest estimator rf's best error=0.1610\n",
      "[flaml.automl.logger: 11-13 21:16:54] {2258} INFO - iteration 49, current learner lgbm\n",
      "[flaml.automl.logger: 11-13 21:16:54] {2442} INFO -  at 17.7s,\testimator lgbm's best error=0.1622,\tbest estimator rf's best error=0.1610\n",
      "[flaml.automl.logger: 11-13 21:16:54] {2258} INFO - iteration 50, current learner rf\n",
      "[flaml.automl.logger: 11-13 21:16:55] {2442} INFO -  at 18.1s,\testimator rf's best error=0.1610,\tbest estimator rf's best error=0.1610\n",
      "[flaml.automl.logger: 11-13 21:16:55] {2258} INFO - iteration 51, current learner extra_tree\n",
      "[flaml.automl.logger: 11-13 21:16:55] {2442} INFO -  at 18.7s,\testimator extra_tree's best error=0.1650,\tbest estimator rf's best error=0.1610\n",
      "[flaml.automl.logger: 11-13 21:16:55] {2258} INFO - iteration 52, current learner lgbm\n",
      "[flaml.automl.logger: 11-13 21:16:56] {2442} INFO -  at 19.0s,\testimator lgbm's best error=0.1622,\tbest estimator rf's best error=0.1610\n",
      "[flaml.automl.logger: 11-13 21:16:56] {2258} INFO - iteration 53, current learner lgbm\n",
      "[flaml.automl.logger: 11-13 21:16:56] {2442} INFO -  at 19.3s,\testimator lgbm's best error=0.1622,\tbest estimator rf's best error=0.1610\n",
      "[flaml.automl.logger: 11-13 21:16:56] {2258} INFO - iteration 54, current learner lgbm\n",
      "[flaml.automl.logger: 11-13 21:16:56] {2442} INFO -  at 19.5s,\testimator lgbm's best error=0.1622,\tbest estimator rf's best error=0.1610\n",
      "[flaml.automl.logger: 11-13 21:16:56] {2258} INFO - iteration 55, current learner lgbm\n",
      "[flaml.automl.logger: 11-13 21:16:56] {2442} INFO -  at 19.8s,\testimator lgbm's best error=0.1622,\tbest estimator rf's best error=0.1610\n",
      "[flaml.automl.logger: 11-13 21:16:56] {2258} INFO - iteration 56, current learner xgboost\n",
      "[flaml.automl.logger: 11-13 21:16:57] {2442} INFO -  at 20.0s,\testimator xgboost's best error=0.1640,\tbest estimator rf's best error=0.1610\n",
      "[flaml.automl.logger: 11-13 21:16:57] {2258} INFO - iteration 57, current learner lgbm\n",
      "[flaml.automl.logger: 11-13 21:16:57] {2442} INFO -  at 20.2s,\testimator lgbm's best error=0.1622,\tbest estimator rf's best error=0.1610\n",
      "[flaml.automl.logger: 11-13 21:16:57] {2258} INFO - iteration 58, current learner rf\n",
      "[flaml.automl.logger: 11-13 21:16:58] {2442} INFO -  at 21.1s,\testimator rf's best error=0.1610,\tbest estimator rf's best error=0.1610\n",
      "[flaml.automl.logger: 11-13 21:16:58] {2258} INFO - iteration 59, current learner catboost\n",
      "[flaml.automl.logger: 11-13 21:17:02] {2442} INFO -  at 24.9s,\testimator catboost's best error=0.1624,\tbest estimator rf's best error=0.1610\n",
      "[flaml.automl.logger: 11-13 21:17:02] {2258} INFO - iteration 60, current learner xgboost\n",
      "[flaml.automl.logger: 11-13 21:17:02] {2442} INFO -  at 25.6s,\testimator xgboost's best error=0.1640,\tbest estimator rf's best error=0.1610\n",
      "[flaml.automl.logger: 11-13 21:17:02] {2258} INFO - iteration 61, current learner lgbm\n",
      "[flaml.automl.logger: 11-13 21:17:03] {2442} INFO -  at 26.0s,\testimator lgbm's best error=0.1622,\tbest estimator rf's best error=0.1610\n",
      "[flaml.automl.logger: 11-13 21:17:03] {2258} INFO - iteration 62, current learner xgboost\n",
      "[flaml.automl.logger: 11-13 21:17:03] {2442} INFO -  at 26.3s,\testimator xgboost's best error=0.1640,\tbest estimator rf's best error=0.1610\n",
      "[flaml.automl.logger: 11-13 21:17:03] {2258} INFO - iteration 63, current learner lgbm\n",
      "[flaml.automl.logger: 11-13 21:17:03] {2442} INFO -  at 26.7s,\testimator lgbm's best error=0.1622,\tbest estimator rf's best error=0.1610\n",
      "[flaml.automl.logger: 11-13 21:17:03] {2258} INFO - iteration 64, current learner xgboost\n",
      "[flaml.automl.logger: 11-13 21:17:04] {2442} INFO -  at 27.2s,\testimator xgboost's best error=0.1640,\tbest estimator rf's best error=0.1610\n",
      "[flaml.automl.logger: 11-13 21:17:04] {2258} INFO - iteration 65, current learner sgd\n",
      "[flaml.automl.logger: 11-13 21:17:04] {2442} INFO -  at 27.3s,\testimator sgd's best error=0.1710,\tbest estimator rf's best error=0.1610\n",
      "[flaml.automl.logger: 11-13 21:17:04] {2258} INFO - iteration 66, current learner lgbm\n",
      "[flaml.automl.logger: 11-13 21:17:04] {2442} INFO -  at 27.5s,\testimator lgbm's best error=0.1622,\tbest estimator rf's best error=0.1610\n",
      "[flaml.automl.logger: 11-13 21:17:04] {2258} INFO - iteration 67, current learner rf\n",
      "[flaml.automl.logger: 11-13 21:17:04] {2442} INFO -  at 27.7s,\testimator rf's best error=0.1610,\tbest estimator rf's best error=0.1610\n",
      "[flaml.automl.logger: 11-13 21:17:04] {2258} INFO - iteration 68, current learner sgd\n",
      "[flaml.automl.logger: 11-13 21:17:04] {2442} INFO -  at 27.8s,\testimator sgd's best error=0.1710,\tbest estimator rf's best error=0.1610\n",
      "[flaml.automl.logger: 11-13 21:17:04] {2258} INFO - iteration 69, current learner xgboost\n",
      "[flaml.automl.logger: 11-13 21:17:05] {2442} INFO -  at 28.0s,\testimator xgboost's best error=0.1640,\tbest estimator rf's best error=0.1610\n",
      "[flaml.automl.logger: 11-13 21:17:05] {2258} INFO - iteration 70, current learner lgbm\n",
      "[flaml.automl.logger: 11-13 21:17:05] {2442} INFO -  at 28.3s,\testimator lgbm's best error=0.1622,\tbest estimator rf's best error=0.1610\n",
      "[flaml.automl.logger: 11-13 21:17:05] {2258} INFO - iteration 71, current learner xgboost\n",
      "[flaml.automl.logger: 11-13 21:17:06] {2442} INFO -  at 28.8s,\testimator xgboost's best error=0.1640,\tbest estimator rf's best error=0.1610\n",
      "[flaml.automl.logger: 11-13 21:17:06] {2258} INFO - iteration 72, current learner extra_tree\n",
      "[flaml.automl.logger: 11-13 21:17:06] {2442} INFO -  at 29.4s,\testimator extra_tree's best error=0.1650,\tbest estimator rf's best error=0.1610\n",
      "[flaml.automl.logger: 11-13 21:17:06] {2258} INFO - iteration 73, current learner xgboost\n",
      "[flaml.automl.logger: 11-13 21:17:06] {2442} INFO -  at 29.7s,\testimator xgboost's best error=0.1635,\tbest estimator rf's best error=0.1610\n",
      "[flaml.automl.logger: 11-13 21:17:06] {2258} INFO - iteration 74, current learner catboost\n",
      "[flaml.automl.logger: 11-13 21:17:11] {2442} INFO -  at 34.0s,\testimator catboost's best error=0.1624,\tbest estimator rf's best error=0.1610\n",
      "[flaml.automl.logger: 11-13 21:17:11] {2258} INFO - iteration 75, current learner extra_tree\n",
      "[flaml.automl.logger: 11-13 21:17:11] {2442} INFO -  at 34.7s,\testimator extra_tree's best error=0.1650,\tbest estimator rf's best error=0.1610\n",
      "[flaml.automl.logger: 11-13 21:17:11] {2258} INFO - iteration 76, current learner catboost\n",
      "[flaml.automl.logger: 11-13 21:17:14] {2442} INFO -  at 37.4s,\testimator catboost's best error=0.1624,\tbest estimator rf's best error=0.1610\n",
      "[flaml.automl.logger: 11-13 21:17:14] {2258} INFO - iteration 77, current learner lgbm\n",
      "[flaml.automl.logger: 11-13 21:17:14] {2442} INFO -  at 37.6s,\testimator lgbm's best error=0.1622,\tbest estimator rf's best error=0.1610\n",
      "[flaml.automl.logger: 11-13 21:17:14] {2258} INFO - iteration 78, current learner lgbm\n",
      "[flaml.automl.logger: 11-13 21:17:14] {2442} INFO -  at 37.8s,\testimator lgbm's best error=0.1622,\tbest estimator rf's best error=0.1610\n",
      "[flaml.automl.logger: 11-13 21:17:14] {2258} INFO - iteration 79, current learner catboost\n",
      "[flaml.automl.logger: 11-13 21:17:19] {2442} INFO -  at 42.7s,\testimator catboost's best error=0.1624,\tbest estimator rf's best error=0.1610\n",
      "[flaml.automl.logger: 11-13 21:17:19] {2258} INFO - iteration 80, current learner rf\n",
      "[flaml.automl.logger: 11-13 21:17:20] {2442} INFO -  at 43.1s,\testimator rf's best error=0.1610,\tbest estimator rf's best error=0.1610\n",
      "[flaml.automl.logger: 11-13 21:17:20] {2258} INFO - iteration 81, current learner lgbm\n",
      "[flaml.automl.logger: 11-13 21:17:20] {2442} INFO -  at 43.4s,\testimator lgbm's best error=0.1622,\tbest estimator rf's best error=0.1610\n",
      "[flaml.automl.logger: 11-13 21:17:20] {2258} INFO - iteration 82, current learner catboost\n",
      "[flaml.automl.logger: 11-13 21:17:23] {2442} INFO -  at 45.8s,\testimator catboost's best error=0.1624,\tbest estimator rf's best error=0.1610\n",
      "[flaml.automl.logger: 11-13 21:17:23] {2258} INFO - iteration 83, current learner rf\n",
      "[flaml.automl.logger: 11-13 21:17:23] {2442} INFO -  at 46.4s,\testimator rf's best error=0.1610,\tbest estimator rf's best error=0.1610\n",
      "[flaml.automl.logger: 11-13 21:17:23] {2258} INFO - iteration 84, current learner extra_tree\n",
      "[flaml.automl.logger: 11-13 21:17:24] {2442} INFO -  at 47.0s,\testimator extra_tree's best error=0.1650,\tbest estimator rf's best error=0.1610\n",
      "[flaml.automl.logger: 11-13 21:17:24] {2258} INFO - iteration 85, current learner extra_tree\n",
      "[flaml.automl.logger: 11-13 21:17:24] {2442} INFO -  at 47.4s,\testimator extra_tree's best error=0.1635,\tbest estimator rf's best error=0.1610\n",
      "[flaml.automl.logger: 11-13 21:17:24] {2258} INFO - iteration 86, current learner catboost\n",
      "[flaml.automl.logger: 11-13 21:17:29] {2442} INFO -  at 52.3s,\testimator catboost's best error=0.1624,\tbest estimator rf's best error=0.1610\n",
      "[flaml.automl.logger: 11-13 21:17:29] {2258} INFO - iteration 87, current learner rf\n",
      "[flaml.automl.logger: 11-13 21:17:30] {2442} INFO -  at 52.8s,\testimator rf's best error=0.1610,\tbest estimator rf's best error=0.1610\n",
      "[flaml.automl.logger: 11-13 21:17:30] {2258} INFO - iteration 88, current learner extra_tree\n",
      "[flaml.automl.logger: 11-13 21:17:30] {2442} INFO -  at 53.3s,\testimator extra_tree's best error=0.1635,\tbest estimator rf's best error=0.1610\n",
      "[flaml.automl.logger: 11-13 21:17:30] {2258} INFO - iteration 89, current learner rf\n",
      "[flaml.automl.logger: 11-13 21:17:31] {2442} INFO -  at 54.0s,\testimator rf's best error=0.1610,\tbest estimator rf's best error=0.1610\n",
      "[flaml.automl.logger: 11-13 21:17:31] {2258} INFO - iteration 90, current learner lgbm\n",
      "[flaml.automl.logger: 11-13 21:17:31] {2442} INFO -  at 54.4s,\testimator lgbm's best error=0.1622,\tbest estimator rf's best error=0.1610\n",
      "[flaml.automl.logger: 11-13 21:17:31] {2258} INFO - iteration 91, current learner catboost\n",
      "[flaml.automl.logger: 11-13 21:17:34] {2442} INFO -  at 57.3s,\testimator catboost's best error=0.1624,\tbest estimator rf's best error=0.1610\n",
      "[flaml.automl.logger: 11-13 21:17:34] {2258} INFO - iteration 92, current learner rf\n",
      "[flaml.automl.logger: 11-13 21:17:34] {2442} INFO -  at 57.6s,\testimator rf's best error=0.1610,\tbest estimator rf's best error=0.1610\n",
      "[flaml.automl.logger: 11-13 21:17:34] {2258} INFO - iteration 93, current learner sgd\n",
      "[flaml.automl.logger: 11-13 21:17:34] {2442} INFO -  at 57.7s,\testimator sgd's best error=0.1710,\tbest estimator rf's best error=0.1610\n",
      "[flaml.automl.logger: 11-13 21:17:34] {2258} INFO - iteration 94, current learner rf\n",
      "[flaml.automl.logger: 11-13 21:17:35] {2442} INFO -  at 58.1s,\testimator rf's best error=0.1610,\tbest estimator rf's best error=0.1610\n",
      "[flaml.automl.logger: 11-13 21:17:35] {2258} INFO - iteration 95, current learner rf\n",
      "[flaml.automl.logger: 11-13 21:17:35] {2442} INFO -  at 58.7s,\testimator rf's best error=0.1610,\tbest estimator rf's best error=0.1610\n",
      "[flaml.automl.logger: 11-13 21:17:35] {2258} INFO - iteration 96, current learner lgbm\n",
      "[flaml.automl.logger: 11-13 21:17:36] {2442} INFO -  at 58.9s,\testimator lgbm's best error=0.1622,\tbest estimator rf's best error=0.1610\n",
      "[flaml.automl.logger: 11-13 21:17:36] {2258} INFO - iteration 97, current learner extra_tree\n",
      "[flaml.automl.logger: 11-13 21:17:36] {2442} INFO -  at 59.3s,\testimator extra_tree's best error=0.1635,\tbest estimator rf's best error=0.1610\n",
      "[flaml.automl.logger: 11-13 21:17:36] {2258} INFO - iteration 98, current learner rf\n",
      "[flaml.automl.logger: 11-13 21:17:36] {2442} INFO -  at 59.7s,\testimator rf's best error=0.1610,\tbest estimator rf's best error=0.1610\n",
      "[flaml.automl.logger: 11-13 21:17:36] {2258} INFO - iteration 99, current learner lgbm\n",
      "[flaml.automl.logger: 11-13 21:17:37] {2442} INFO -  at 59.9s,\testimator lgbm's best error=0.1622,\tbest estimator rf's best error=0.1610\n",
      "[flaml.automl.logger: 11-13 21:17:37] {2258} INFO - iteration 100, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-13 21:17:37] {2442} INFO -  at 60.0s,\testimator xgb_limitdepth's best error=0.1731,\tbest estimator rf's best error=0.1610\n",
      "[flaml.automl.logger: 11-13 21:17:37] {2685} INFO - retrain rf for 0.1s\n",
      "[flaml.automl.logger: 11-13 21:17:37] {2688} INFO - retrained model: RandomForestClassifier(criterion='entropy', max_features=0.5271736421348838,\n",
      "                       max_leaf_nodes=25, n_estimators=11, n_jobs=-1,\n",
      "                       random_state=12032022)\n",
      "[flaml.automl.logger: 11-13 21:17:37] {1985} INFO - fit succeeded\n",
      "[flaml.automl.logger: 11-13 21:17:37] {1986} INFO - Time taken to find the best model: 4.835913896560669\n",
      "The evaluation score is 0.81298\n"
     ]
    }
   ],
   "source": [
    "titanic = fetch_openml('titanic', version=1, as_frame=True)\n",
    "data = titanic.frame\n",
    "features = [\"pclass\", \"sex\", \"sibsp\", \"parch\", \"embarked\"]\n",
    "target = \"survived\"\n",
    "X = data[features]\n",
    "y = data[target]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "X_train = pd.get_dummies(X_train[features])\n",
    "X_test  = pd.get_dummies(X_test[features])\n",
    "\n",
    "automl = AutoML()\n",
    "automl.fit(X_train, y_train, time_budget=60, task=\"classification\")\n",
    "automl_cont = AutoML()\n",
    "automl_cont.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    time_budget=60,\n",
    "    task=\"classification\",\n",
    "    starting_points=automl.best_config_per_estimator,\n",
    ")\n",
    "\n",
    "y_hat = automl_cont.predict(X_test)\n",
    "eval_metric = accuracy_score(y_test , y_hat)\n",
    "print(\"The evaluation score is %.5f\" % eval_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III. Retrieving the Outcome\n",
    "\n",
    "We can obtain the best model this way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PGAO\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\datasets\\_openml.py:1002: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<flaml.automl.model.XGBoostSklearnEstimator object at 0x000001E76D53DB70>\n",
      "XGBClassifier(base_score=None, booster=None, callbacks=[],\n",
      "              colsample_bylevel=0.9706603459860277, colsample_bynode=None,\n",
      "              colsample_bytree=0.8454846704595121, device=None,\n",
      "              early_stopping_rounds=None, enable_categorical=False,\n",
      "              eval_metric=None, feature_types=None, gamma=None,\n",
      "              grow_policy='lossguide', importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=1.0, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=0, max_leaves=4,\n",
      "              min_child_weight=0.8021745972157645, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=4,\n",
      "              n_jobs=-1, num_parallel_tree=None, random_state=None, ...)\n",
      "xgboost\n",
      "{'n_estimators': 4, 'max_leaves': 4, 'min_child_weight': 0.8021745972157645, 'learning_rate': 1.0, 'subsample': 0.923723172598251, 'colsample_bylevel': 0.9706603459860277, 'colsample_bytree': 0.8454846704595121, 'reg_alpha': 0.0015786559512178317, 'reg_lambda': 1.572596754658357}\n",
      "{'lgbm': {'n_estimators': 11, 'num_leaves': 4, 'min_child_samples': 12, 'learning_rate': 0.8254822443934743, 'log_max_bin': 7, 'colsample_bytree': 0.8470841969798473, 'reg_alpha': 0.006515501057368674, 'reg_lambda': 0.40106918060159397}, 'rf': {'n_estimators': 4, 'max_features': 0.3437262012124751, 'max_leaves': 10, 'criterion': 'entropy'}, 'xgboost': {'n_estimators': 4, 'max_leaves': 4, 'min_child_weight': 0.8021745972157645, 'learning_rate': 1.0, 'subsample': 0.923723172598251, 'colsample_bylevel': 0.9706603459860277, 'colsample_bytree': 0.8454846704595121, 'reg_alpha': 0.0015786559512178317, 'reg_lambda': 1.572596754658357}, 'extra_tree': {'n_estimators': 8, 'max_features': 0.5180514177504905, 'max_leaves': 6, 'criterion': 'gini'}, 'xgb_limitdepth': {'n_estimators': 10, 'max_depth': 6, 'min_child_weight': 0.9999999999999993, 'learning_rate': 0.29999999999999993, 'subsample': 1.0, 'colsample_bylevel': 1.0, 'colsample_bytree': 1.0, 'reg_alpha': 0.0009765625, 'reg_lambda': 1.0}, 'sgd': {'penalty': 'l1', 'alpha': 1.9347936808915984e-05, 'l1_ratio': 0.003105261192898503, 'epsilon': 0.1, 'learning_rate': 'constant', 'eta0': 0.021090835070720838, 'power_t': 0.45860163885033806, 'average': False, 'loss': 'log_loss'}, 'catboost': {'early_stopping_rounds': 12, 'learning_rate': 0.1604199347807429, 'n_estimators': 8192}, 'lrl1': None}\n",
      "0.022838115692138672\n",
      "80\n",
      "0.19862383230804284\n",
      "25.28597331047058\n"
     ]
    }
   ],
   "source": [
    "automl_settings = {\n",
    "    \"time_budget\": 30,  # in seconds\n",
    "    \"metric\": \"accuracy\",\n",
    "    \"task\": \"classification\",\n",
    "    \"log_file_name\": \"mylog.log\",\n",
    "    \"verbose\": False}\n",
    "\n",
    "titanic = fetch_openml('titanic', version=1, as_frame=True)\n",
    "data = titanic.frame\n",
    "features = [\"pclass\", \"sex\", \"sibsp\", \"parch\", \"embarked\"]\n",
    "target = \"survived\"\n",
    "X = data[features]\n",
    "y = data[target]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "X_train = pd.get_dummies(X_train[features])\n",
    "X_test  = pd.get_dummies(X_test[features])\n",
    "\n",
    "automl = AutoML()\n",
    "automl.fit(X_train, y_train, **automl_settings)\n",
    "print(automl.model)\n",
    "print(automl.model.estimator)\n",
    "print(automl.best_estimator)\n",
    "print(automl.best_config)\n",
    "print(automl.best_config_per_estimator)\n",
    "print(automl.best_config_train_time)\n",
    "print(automl.best_iteration)\n",
    "print(automl.best_loss)\n",
    "print(automl.time_to_find_best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get the learning curve. To plot how the loss is improved over time during the model search, first load the search history from the log file. Then, we can plot the metric versus wallclock time. The curve below suggests that increasing the time budget may not further improve the accuracy any more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "time_history, best_valid_loss_history, valid_loss_history, config_history, metric_history = get_output_from_log(filename=automl_settings[\"log_file_name\"], time_budget=120)\n",
    "plt.title(\"Learning Curve\")\n",
    "plt.xlabel(\"Wall Clock Time (s)\")\n",
    "plt.ylabel(\"Validation Metric (Accuracy)\")\n",
    "plt.step(time_history, 1 - np.array(best_valid_loss_history), where=\"post\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How much Time is Needed\n",
    "\n",
    "If you want to get a sense of how much time is needed to find the best model, you can use _max\\_iter_=2 to perform two trials first. The message will be like:\n",
    "\n",
    "     INFO - iteration 0, current learner lgbm\n",
    "     INFO - Estimated sufficient time budget=145194s. Estimated necessary time budget=2118s.\n",
    "     INFO - at 2.6s, estimator lgbm's best error=0.4459, best estimator lgbm's best error=0.4459\n",
    "\n",
    "You will see that the time to finish the first and cheapest trial is 2.6 seconds. The estimated necessary time budget is 2118 seconds, and the estimated sufficient time budget is 145194 seconds. Note that this is only an estimated range to help you decide your budget.\n",
    "\n",
    "When the time budget is set too low, it can happen that no estimator is trained at all within the budget. In this case, it is recommanded to use _max\\_iter_ instead of _time\\_budget_. This ensures that you have enough time to train a model without worring about variance of the execution time for the code before starting a trainning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### References:\n",
    "\n",
    "   - https://microsoft.github.io/FLAML/docs/Getting-Started\n",
    "   - https://microsoft.github.io/FLAML/docs/Installation\n",
    "   - https://microsoft.github.io/FLAML/docs/Use-Cases/Task-Oriented-AutoML\n",
    "   - https://github.com/microsoft/FLAML/blob/main/notebook/basics/understanding_cross_validation.ipynb\n",
    "   - https://arxiv.org/abs/1109.0887\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python310_uat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
